---
title: "DATA495 Weekly Report 4"
format: html
---

# Weekly Report 4
Date: February 9th, 2024

## Past Objective Status
Objective|Description|Status
----|-------|---
Literature Review|Explainable/Trustworthy AI in Medicine - what are the requirements and how can we build models that meet these requirements| Complete, see below
Elements of Statistical Learning|Discuss topics with Devan in weekly meeting
Application 2|Due Monday night, complete analysis of boosting and mars models.

## Weekly Work Log
Activity|Description|Time Allocated
----|------|--:
Application 2|Boosting and zero inflated models |3 hours
Elements of Statistical Learning|Chapter 9.2 Tree-Based Methods|45 minutes
Literature Review|See below|1 hour

## Upcoming Objectives
Objective|Description
----|-------
Literature Review|Explainable tree based methods
Elements of Statistical Learning|9.3 Prim Bump Hunting
Application 3| Implement explainable trees
Midterm Knowledge Report|Prepare infographics

## Literature Review
Title|DOI|Comments
---|--|-------
Improving the explainability of Random Forest classifier â€“ user centered approach|https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728671/|This article is extremely relevant to this course and my future interests in ML in the health care industry. It outlines the process of creating explainable machine learning tools for use by industry professionals. The authors began their work with discussions with practitioners regarding their expectations for relying on AI in their work. The authors created a one-page graphic that addressing all the expectations and enhances the decisions made by practitioners. I really like this article because it provides a framework for how we can incorporate AI into the bio sciences.
Multivariate adaptive regression splines analysis to predict biomarkers of spontaneous preterm birth|1600-0412|I found this article because it was cited the previous article and I wanted to find further extensions of explainable interfaces in the context of biosciences, specifically genetics. These authors attempted to predict pig slaughter age using phenotype factors and although successful, did not explicitly create a explainability interface. They included a graph of the feature importances but this does not solve the problem of making machine learning results understandable for non-expert users from the industry. They only referenced the previous article in the discussion as a framework for enhancing explainability.