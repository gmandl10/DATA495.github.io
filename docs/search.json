[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "1 Introduction",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html",
    "href": "Applications/Application1/EDA.html",
    "title": "2  Data Pre-Processing",
    "section": "",
    "text": "2.1 DS0214\nDietary Supplement Use – Supplement Information\nThis data set stores dietary supplement information. The primary key to this dataset is the Supplement ID number. Let’s see if this is a foreign key in another dataset\nfor i in dataframes.index:\n    df = dataframes[i]\n    \n    if \"DSDSUPID\" in df.columns:\n        print(i)\n\nDS0213\nDS0214\nDS0215\nThe Supplement ID number (DSDSUPID) is the primary key in DS0214 and is a foreign key in DS0213\nDS0213 is a dataset that contains the serial number and the data release number. DSO213 is Dietary Supplement Use – Participants Use of Supplement Let’s see how DSO213 is structured: Can one person be recorded taking multiple supplements? If so, what is the distribution of supplement usage in the same?\nDS2013 = pd.read_csv(\"ICPSR_25504\\\\DS0213\\\\25504-0213-Data.tsv\", sep = \"\\t\")\nsupplement_usage = DS2013.groupby(\"SEQN\").size().sort_values(ascending = False).reset_index(name=\"Count\")\nsupplement_usage\n\n\n\n\n\n\n\n\nSEQN\nCount\n\n\n\n\n0\n34656\n20\n\n\n1\n40551\n19\n\n\n2\n37134\n16\n\n\n3\n38637\n16\n\n\n4\n37164\n14\n\n\n...\n...\n...\n\n\n4105\n35518\n1\n\n\n4106\n35522\n1\n\n\n4107\n35524\n1\n\n\n4108\n35525\n1\n\n\n4109\n36307\n1\n\n\n\n\n4110 rows × 2 columns\nimport matplotlib.pyplot as plt\nplt.hist(supplement_usage[\"Count\"], bins = 12)\nplt.xlabel(\"Number of Supplements Used\")\nplt.ylabel(\"Number of Participants\")\nplt.title(\"Distribution of Supplement Usage Among Study Participants\")\nplt.show()\nA key thing to note is that a participant who is not using supplements is not included in this visualization. This analysis is only representative of a participant that uses one or multiple supplement(s)\nLet’s look at the next dataset that does not contain the sequence number or data release number",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#ds0215",
    "href": "Applications/Application1/EDA.html#ds0215",
    "title": "2  Data Pre-Processing",
    "section": "2.2 DS0215",
    "text": "2.2 DS0215\nDietary Supplement Use – Ingredient Information\nThe Supplement ID number (DSDSUPID) is also the primary key in DS0214 and is a foreign key in DS0213\nCan a supplement have several ingredients and what is the distributions of number ingredients in a supplements\n\nDS2015 = pd.read_csv(\"ICPSR_25504\\\\DS0215\\\\25504-0215-Data.tsv\", sep = \"\\t\")\nnum_ingredients = DS2015.groupby(\"DSDSUPP\").size().sort_values(ascending = False).reset_index(name=\"Count\")\nnum_ingredients\n\n\n\n\n\n\n\n\nDSDSUPP\nCount\n\n\n\n\n0\nS1000579801\n68\n\n\n1\nS1000579800\n65\n\n\n2\nS1000580200\n65\n\n\n3\nS1000600500\n65\n\n\n4\nS1000582500\n64\n\n\n...\n...\n...\n\n\n2138\nS1000649000\n1\n\n\n2139\nS1000649100\n1\n\n\n2140\nS1000649200\n1\n\n\n2141\nS1000649300\n1\n\n\n2142\nS1888690300\n1\n\n\n\n\n2143 rows × 2 columns\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.hist(num_ingredients[\"Count\"], bins = 12)\nplt.xlabel(\"Number of Ingredients in Supplement\")\nplt.ylabel(\"Number of Supplements\")\nplt.title(\"Distribution of Amount of Ingredients Among Supplements\")\nplt.show()\n\n\n\n\n\n\n\n\n\nDS2015[DS2015[\"DSDSUPP\"]==\"S1000579801\"]\n\n\n\n\n\n\n\n\nDSDSUPID\nDSDSUPP\nDSDINGID\nDSDINGR\nDSDOPER\nDSDQTY\nDSDUNIT\nDSDCAT\nDSDBLFLG\n\n\n\n\n11661\n1000579801\nS1000579801\n10000037\nI10000037\n=\n100.0\n1\n4\n2\n\n\n11662\n1000579801\nS1000579801\n10000038\nI10000038\n=\n50.0\n1\n4\n2\n\n\n11663\n1000579801\nS1000579801\n10000042\nI10000042\n=\n150.0\n4\n1\n2\n\n\n11664\n1000579801\nS1000579801\n10000052\nI10000052\n=\n1.5\n1\n2\n2\n\n\n11665\n1000579801\nS1000579801\n10000070\nI10000070\n=\n250.0\n1\n2\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11724\n1000579801\nS1000579801\n10004592\nI10004592\n=\n400.0\n1\n4\n2\n\n\n11725\n1000579801\nS1000579801\n10004593\nI10004593\n=\n100.0\n1\n4\n2\n\n\n11726\n1000579801\nS1000579801\n10004595\nI10004595\n=\n100.0\n1\n4\n2\n\n\n11727\n1000579801\nS1000579801\n10004927\nI10004927\n=\n15.0\n1\n3\n2\n\n\n11728\n1000579801\nS1000579801\n10004928\nI10004928\n=\n125.0\n1\n4\n2\n\n\n\n\n68 rows × 9 columns",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#ds0216",
    "href": "Applications/Application1/EDA.html#ds0216",
    "title": "2  Data Pre-Processing",
    "section": "2.3 DS0216",
    "text": "2.3 DS0216\nDietary Supplement Use – Supplement Blend\nThe primary key in DS0216 is the Ingredient ID number which is a foreign key in DS0215",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#ds0235",
    "href": "Applications/Application1/EDA.html#ds0235",
    "title": "2  Data Pre-Processing",
    "section": "2.4 DS0235",
    "text": "2.4 DS0235\nDrug Information\nThe primary key for DS0235 is the Generic Drug Code (RXDDRGID). Let’s find which dataset has the generic drug code as a foreign key.\n\nfor i in dataframes.index:\n    df = dataframes[i]\n    \n    if \"RXDDRGID\" in df.columns:\n        print(i)\n\nDS0234\nDS0235\n\n\nDS0234 contains RXDDRGID as a foreign key. This dataset is Prescription Medications\n\nDS2034 = pd.read_csv(\"ICPSR_25504\\\\DS0234\\\\25504-0234-Data.tsv\", sep = \"\\t\")\nnum_drugs = DS2034.groupby(\"SEQN\").size().sort_values(ascending = False).reset_index(name=\"Count\")\nnum_drugs\n\n\n\n\n\n\n\n\nSEQN\nCount\n\n\n\n\n0\n41025\n20\n\n\n1\n34531\n18\n\n\n2\n34423\n17\n\n\n3\n38612\n16\n\n\n4\n33959\n16\n\n\n...\n...\n...\n\n\n10343\n34985\n1\n\n\n10344\n34984\n1\n\n\n10345\n34983\n1\n\n\n10346\n34982\n1\n\n\n10347\n36301\n1\n\n\n\n\n10348 rows × 2 columns\n\n\n\n\n# Partipicant 41025 uses 20 perscription drugs\nDS2034[DS2034[\"SEQN\"] == 41025]\n\n\n\n\n\n\n\n\nSEQN\nRXDUSE\nRXDDRUG\nRXDDRGID\nRXQSEEN\nRXDDAYS\nRXDCOUNT\nSDDSRVYR\nRIDSTATR\nRIDEXMON\n...\nFIAPROXY\nFIAINTRP\nMIALANG\nMIAPROXY\nMIAINTRP\nAIALANG\nWTINT2YR\nWTMEC2YR\nSDMVPSU\nSDMVSTRA\n\n\n\n\n17125\n41025\n1\nACETAMINOPHEN; HYDROCODONE\nd03428\n1\n1825\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17126\n41025\n1\nAZELASTINE NASAL\nd04068\n1\n182\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17127\n41025\n1\nBUSPIRONE\nd00182\n1\n122\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17128\n41025\n1\nCHOLESTYRAMINE\nd00193\n1\n547\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17129\n41025\n1\nCITALOPRAM\nd04332\n1\n243\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17130\n41025\n1\nDILTIAZEM\nd00045\n1\n365\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17131\n41025\n1\nESOMEPRAZOLE\nd04749\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17132\n41025\n1\nFLUTICASONE\nd01296\n1\n1825\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17133\n41025\n1\nFLUTICASONE; SALMETEROL\nd04611\n1\n1095\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17134\n41025\n1\nGABAPENTIN\nd03182\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17135\n41025\n1\nGUAIFENESIN; PSEUDOEPHEDRINE\nd03379\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17136\n41025\n1\nLORAZEPAM\nd00149\n1\n365\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17137\n41025\n1\nMETAXALONE\nd00964\n1\n1460\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17138\n41025\n1\nNABUMETONE\nd00310\n1\n243\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17139\n41025\n1\nOXYBUTYNIN\nd00328\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17140\n41025\n1\nOXYCODONE\nd00329\n1\n1825\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17141\n41025\n1\nQUININE\nd00366\n1\n10950\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17142\n41025\n1\nSUMATRIPTAN\nd03160\n1\n1825\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17143\n41025\n1\nTAMSULOSIN\nd04121\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17144\n41025\n1\nTRAZODONE\nd00395\n1\n365\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n\n\n20 rows × 49 columns\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.hist(num_drugs[\"Count\"], bins = 10)\nplt.xlabel(\"Number of Perscription Drugs Used\")\nplt.ylabel(\"Number of Participants\")\nplt.title(\"Perpsection Drug Usage Among Study Participants\")\nplt.show()",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#year-to-year-survey-data-consistency",
    "href": "Applications/Application1/EDA.html#year-to-year-survey-data-consistency",
    "title": "2  Data Pre-Processing",
    "section": "2.5 Year to Year Survey Data Consistency",
    "text": "2.5 Year to Year Survey Data Consistency\nThe first data that we have been working with from the dataset ICPSR_25504 is the National Health and Nutrition Examination Survey (NHANES) from 2005-2006. Next we have downloaded the Survey from 2005-2006 and we will look to see if the data architecture is consistent across year.\nFirst let’s check if each data set exists in both surveys and if their columns are the same\n\nimport os\nimport pandas as pd\n\ndata_master = \"ICPSR_25505\\\\\"\ndataframes2 = []\nindex2 = []\ni = 0\n\nos.listdir(data_master)\nfor root, dirs, files in os.walk(data_master):\n    for name in files:\n        if name.endswith(\".tsv\"):\n            file = os.path.join(root, name)\n            dataframes2.append(pd.read_csv(file, sep = \"\\t\", nrows = 100))\n            index2.append(root[-6:])\n\ndataframes2 = pd.Series(dataframes2, index = index2)\n\n\nprint(\"Datasets included in 2005-2006 but not in 2007-2008\")\nfor i in index:\n    if i not in index2:\n        print(i)\n\n\nprint(\"Datasets not included in 2005-2006 but in 2007-2008\")\nfor i in index2:\n    if i not in index:\n        print(i)\n\nprint(\"Datasets with mismatching columns\")\nfor i in index:\n    if i in index2:\n        if len(dataframes[i].columns) != len(dataframes2[i].columns):\n            print(i)\n        \nprint(\"Datasets with consistent number of columns\")\nfor i in index:\n    if i in index2:\n        if len(dataframes[i].columns) == len(dataframes2[i].columns):\n            print(i)\n        \n\nDatasets included in 2005-2006 but not in 2007-2008\nDS0018\nDS0019\nDS0020\nDS0021\nDS0022\nDS0023\nDS0024\nDS0025\nDS0026\nDS0128\nDS0129\nDS0130\nDS0131\nDS0132\nDS0133\nDS0134\nDS0135\nDS0136\nDS0137\nDS0138\nDS0139\nDS0140\nDS0141\nDS0142\nDS0143\nDS0144\nDS0234\nDS0235\nDS0236\nDS0237\nDS0238\nDS0239\nDS0240\nDS0241\nDS0242\nDS0243\nDS0244\nDS0245\nDS0246\nDS0247\nDS0248\nDatasets not included in 2005-2006 but in 2007-2008\nDS0100\nDatasets with mismatching columns\nDS0011\nDS0012\nDS0013\nDS0014\nDS0015\nDS0016\nDS0017\nDS0101\nDS0102\nDS0103\nDS0104\nDS0105\nDS0106\nDS0107\nDS0108\nDS0109\nDS0110\nDS0111\nDS0113\nDS0114\nDS0115\nDS0116\nDS0117\nDS0118\nDS0119\nDS0120\nDS0121\nDS0122\nDS0123\nDS0124\nDS0125\nDS0126\nDS0127\nDS0203\nDS0204\nDS0206\nDS0207\nDS0208\nDS0209\nDS0210\nDS0211\nDS0212\nDS0214\nDS0215\nDS0216\nDS0217\nDS0218\nDS0219\nDS0220\nDS0221\nDS0222\nDS0223\nDS0224\nDS0225\nDS0226\nDS0227\nDS0228\nDS0229\nDS0230\nDS0231\nDS0232\nDS0233\nDatasets with consistent number of columns\nDS0001\nDS0112\nDS0201\nDS0202\nDS0205\nDS0213\n\n\n\nname = \"DS0001\"\ni=0\n\nwhile i &lt; len(dataframes[name].columns):\n    if dataframes[name].columns[i] in dataframes2[name].columns:\n        i += 1\n    else:\n        break\n\nif i == len(dataframes[name].columns):\n    print(\"True\")\nelse:\n    print(\"False\")\n\nFalse\n\n\n\ndataframes[name].columns,dataframes2[name].columns\n\n(Index(['SEQN', 'SDDSRVYR', 'RIDSTATR', 'RIDEXMON', 'RIAGENDR', 'RIDAGEYR',\n        'RIDAGEMN', 'RIDAGEEX', 'RIDRETH1', 'DMQMILIT', 'DMDBORN', 'DMDCITZN',\n        'DMDYRSUS', 'DMDEDUC3', 'DMDEDUC2', 'DMDSCHOL', 'DMDMARTL', 'DMDHHSIZ',\n        'DMDFMSIZ', 'INDHHINC', 'INDFMINC', 'INDFMPIR', 'RIDEXPRG', 'DMDHRGND',\n        'DMDHRAGE', 'DMDHRBRN', 'DMDHREDU', 'DMDHRMAR', 'DMDHSEDU', 'SIALANG',\n        'SIAPROXY', 'SIAINTRP', 'FIALANG', 'FIAPROXY', 'FIAINTRP', 'MIALANG',\n        'MIAPROXY', 'MIAINTRP', 'AIALANG', 'WTINT2YR', 'WTMEC2YR', 'SDMVPSU',\n        'SDMVSTRA'],\n       dtype='object'),\n Index(['SEQN', 'SDDSRVYR', 'RIDSTATR', 'RIDEXMON', 'RIAGENDR', 'RIDAGEYR',\n        'RIDAGEMN', 'RIDAGEEX', 'RIDRETH1', 'DMQMILIT', 'DMDBORN2', 'DMDCITZN',\n        'DMDYRSUS', 'DMDEDUC3', 'DMDEDUC2', 'DMDSCHOL', 'DMDMARTL', 'DMDHHSIZ',\n        'DMDFMSIZ', 'INDHHIN2', 'INDFMIN2', 'INDFMPIR', 'RIDEXPRG', 'DMDHRGND',\n        'DMDHRAGE', 'DMDHRBR2', 'DMDHREDU', 'DMDHRMAR', 'DMDHSEDU', 'SIALANG',\n        'SIAPROXY', 'SIAINTRP', 'FIALANG', 'FIAPROXY', 'FIAINTRP', 'MIALANG',\n        'MIAPROXY', 'MIAINTRP', 'AIALANG', 'WTINT2YR', 'WTMEC2YR', 'SDMVPSU',\n        'SDMVSTRA'],\n       dtype='object'))\n\n\nColumns are almost identical aside from DMDBORN and DMDBORN2\nLet’s see how many participants we have from both surveys\n\nICPSR_25504 = pd.read_csv(\"ICPSR_25504\\\\DS0001\\\\25504-0001-Data.tsv\", sep = \"\\t\")\nICPSR_25505 = pd.read_csv(\"ICPSR_25505\\\\DS0001\\\\25505-0001-Data.tsv\", sep = \"\\t\")\nICPSR_25504[\"SEQN\"].apply(lambda x: x in ICPSR_25505[\"SEQN\"].values).sum()\n\n0\n\n\nThere is no way of identifying previous participants in new surverys. There also seems to be little value in analyzing multiple surveys because we can’t create variables that look at year to year changes in the same patient and the data architecture is not consistent across years. The only potential uses of having surveys from different years is to see if a model built on data from one year generalizes to the survey data from other years. In order to do some we must choose variables and datasets that exist in all surveys.",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#joining-datasets",
    "href": "Applications/Application1/EDA.html#joining-datasets",
    "title": "2  Data Pre-Processing",
    "section": "2.6 Joining Datasets",
    "text": "2.6 Joining Datasets\n\nimport pandas as pd\nDS0132= pd.read_csv(\"ICPSR_25504\\\\DS0132\\\\25504-0132-Data.tsv\", sep = \"\\t\")\nDS0123 = pd.read_csv(\"ICPSR_25504\\\\DS0123\\\\25504-0123-Data.tsv\", sep = \"\\t\")\njoined = DS0132.set_index(\"SEQN\").join(DS0123.set_index(\"SEQN\"), lsuffix=\"_DS0132\", rsuffix=\"_DS0123\")\n\n\njoined\n\n\n\n\n\n\n\n\nWTSAF2YR_DS0132\nLBXTR\nLBDTRSI\nLBDLDL\nLBDLDLSI\nLBXAPB\nLBDAPBSI\nSDDSRVYR_DS0132\nRIDSTATR_DS0132\nRIDEXMON_DS0132\n...\nFIAPROXY_DS0123\nFIAINTRP_DS0123\nMIALANG_DS0123\nMIAPROXY_DS0123\nMIAINTRP_DS0123\nAIALANG_DS0123\nWTINT2YR_DS0123\nWTMEC2YR_DS0123\nSDMVPSU_DS0123\nSDMVSTRA_DS0123\n\n\nSEQN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n31130\n0.000\n\n\n\n\n\n\n4\n2\n2\n...\n2\n2\n\n\n\n\n29960.839509\n34030.994786\n2\n46\n\n\n31131\n67556.810\n86\n.971\n49\n1.267\n50\n.5\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n26457.708180\n26770.584605\n1\n48\n\n\n31132\n80193.962\n65\n.734\n75\n1.94\n75\n.75\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n32961.509920\n35315.538900\n2\n52\n\n\n31133\n15668.017\n61\n.689\n81\n2.095\n75\n.75\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n5635.221296\n5920.617679\n1\n51\n\n\n31134\n93399.539\n195\n2.202\n98\n2.534\n111\n1.11\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n43718.506372\n44231.167252\n2\n48\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41465\n21859.428\n81\n.914\n96\n2.483\n84\n.84\n4\n2\n1\n...\n2\n2\n1\n2\n2\n1\n6659.491266\n6975.174925\n1\n49\n\n\n41467\n8000.729\n69\n.779\n80\n2.069\n75\n.75\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n3623.884248\n3780.835721\n2\n56\n\n\n41471\n16274.316\n89\n1.005\n72\n1.862\n60\n.6\n4\n2\n1\n...\n2\n2\n1\n2\n2\n1\n6283.305550\n6602.173657\n1\n52\n\n\n41472\n175395.280\n120\n1.355\n86\n2.224\n91\n.91\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n67347.579152\n69747.141506\n1\n48\n\n\n41474\n17122.983\n88\n.994\n35\n.905\n37\n.37\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n6174.551667\n6487.262494\n1\n47\n\n\n\n\n3352 rows × 98 columns\n\n\n\nLet’s see if the columns that occur in both dataframes are redundant\n\n(joined[\"AIALANG_DS0123\"] == joined[\"AIALANG_DS0132\"]).mean()\n# AIALANG is definitely redundant\n\n1.0\n\n\n\nDS1 = \"DS0123\"\nDS2 = \"DS0132\"\n\nn = 0\ntotal = 0\nfor column in joined.columns:\n\n    if \"_\" in column:\n        total +=1\n        original = column.split(\"_\")[0]\n        n1 = original + \"_\" + DS1\n        n2 = original + \"_\" + DS2\n\n        if (joined[n1]==joined[n2]).mean() == 1:\n            n += 1\n            joined.drop(n2, axis = 1)\n\nprint(str(n/total) + \" of the columns that occur in both datasets are redundant\")\n        \n\n1.0 of the columns that occur in both datasets are redundant\n\n\n\nimport os\nimport pandas as pd\n\ndata_master = \"ICPSR_25504\\\\\"\ni = 0\njoined_master = DS0123.set_index(\"SEQN\")\n\nfor root, dirs, files in os.walk(data_master):\n    for name in files:\n        if name.endswith(\".tsv\"):\n            file = os.path.join(root, name)\n            df = pd.read_csv(file, sep = \"\\t\")\n            if \"SEQN\" not in df.columns:\n                continue\n            name = root[-6:]\n        \n            new_columns = df.columns.difference(joined_master.columns)\n            # try:\n            joined_master = joined_master.join(df.loc[:,new_columns].set_index(\"SEQN\"),how=\"left\",lsuffix=\"_master\", rsuffix=\"_\"+name)\n            # except:\n            #     print(name)\n            # n = 0\n            # total = 0\n            # for column in joined_master.columns:\n            #     if \"_\" in column:\n            #         print(column, name)\n            #         total +=1\n            #         original = column.split(\"_\")[0]\n            #         n1 = original + \"_master\"\n            #         n2 = original + \"_\" + name\n\n            #         if n2 not in joined_master.columns:\n            #             continue\n\n            #         if (joined_master[n1]==joined_master[n2]).mean() == 1:\n            #             n += 1\n            #             joined_master.drop(n2, axis = 1)\n            #             joined_master.rename({n1:original})        \n            # if n/total != 1:\n            #     print(name + \" - not redundant\")\n            \n\nMemoryError: Unable to allocate 1.30 GiB for an array with shape (222, 786976) and data type float64\n\n\n\njoined_master\n\n\n\n\n\n\n\n\nWTSAF2YR\nLBXGLU\nLBDGLUSI\nLBXIN\nLBDINSI\nPHAFSTHR\nPHAFSTMN\nSDDSRVYR\nRIDSTATR\nRIDEXMON\n...\nDXXNKBMD\nDXXOFA\nDXXOFBMC\nDXXOFBMD\nDXXTRA\nDXXTRBMC\nDXXTRBMD\nDXXWDA\nDXXWDBMC\nDXXWDBMD\n\n\nSEQN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n31130\n0.000\n\n\n\n\n10\n3\n4\n2\n2\n...\n\n\n\n\n\n\n\n\n\n\n\n\n31131\n67556.810\n90\n4.996\n10.03\n60.18\n14\n9\n4\n2\n2\n...\n\n\n\n\n\n\n\n\n\n\n\n\n31131\n67556.810\n90\n4.996\n10.03\n60.18\n14\n9\n4\n2\n2\n...\n\n\n\n\n\n\n\n\n\n\n\n\n31131\n67556.810\n90\n4.996\n10.03\n60.18\n14\n9\n4\n2\n2\n...\n\n\n\n\n\n\n\n\n\n\n\n\n31131\n67556.810\n90\n4.996\n10.03\n60.18\n14\n9\n4\n2\n2\n...\n\n\n\n\n\n\n\n\n\n\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41474\n17122.983\n93\n5.162\n20.12\n120.72\n9\n39\n4\n2\n2\n...\n.825\n38.34\n30.73\n.802\n12.1\n6.63\n.548\n1.3\n.96\n.736\n\n\n41474\n17122.983\n93\n5.162\n20.12\n120.72\n9\n39\n4\n2\n2\n...\n.825\n38.34\n30.73\n.802\n12.1\n6.63\n.548\n1.3\n.96\n.736\n\n\n41474\n17122.983\n93\n5.162\n20.12\n120.72\n9\n39\n4\n2\n2\n...\n.825\n38.34\n30.73\n.802\n12.1\n6.63\n.548\n1.3\n.96\n.736\n\n\n41474\n17122.983\n93\n5.162\n20.12\n120.72\n9\n39\n4\n2\n2\n...\n.825\n38.34\n30.73\n.802\n12.1\n6.63\n.548\n1.3\n.96\n.736\n\n\n41474\n17122.983\n93\n5.162\n20.12\n120.72\n9\n39\n4\n2\n2\n...\n.825\n38.34\n30.73\n.802\n12.1\n6.63\n.548\n1.3\n.96\n.736\n\n\n\n\n786976 rows × 742 columns\n\n\n\n\n# The data is far too large to combine into one so I will choose critical datasets to focus the analysis on",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#ds209---depression-screener",
    "href": "Applications/Application1/EDA.html#ds209---depression-screener",
    "title": "2  Data Pre-Processing",
    "section": "2.7 DS209 - Depression Screener",
    "text": "2.7 DS209 - Depression Screener\nThis data set is a Questionnaire with 10 questions about depressive symptoms (Eg. Thought you would be better off dead.) The response are categorical from 0-3 with 0 being non-depressive behaviour (Not at all) and 4 being depressive behaviour (Nearly every day).\nWe will create a target variable that aggregate the results of these questions, taking the mean response of the questions which scores a participants depressive state. 0 meaning low, 4 high\nThe respondant can answer a question as “Don’t know” and will be assigned the number 9 which can throw off our scores. I will remove these values from the average calculations. So for example, if a respondant answers “Don’t know” for 2 questions, their score will be the average of their repsonses to the other 8 questions.\n\nDS209= pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"\\\\Data\\\\NHANES\\\\ICPSR_25504\\DS0209\\\\25504-0209-Data.tsv\", sep = \"\\t\")\n\n\nDS209.dtypes\n\nSEQN          int64\nDPQ010       object\nDPQ020       object\nDPQ030       object\nDPQ040       object\nDPQ050       object\nDPQ060       object\nDPQ070       object\nDPQ080       object\nDPQ090       object\nDPQ100       object\nSDDSRVYR      int64\nRIDSTATR      int64\nRIDEXMON      int64\nRIAGENDR      int64\nRIDAGEYR      int64\nRIDAGEMN     object\nRIDAGEEX     object\nRIDRETH1      int64\nDMQMILIT      int64\nDMDBORN       int64\nDMDCITZN      int64\nDMDYRSUS     object\nDMDEDUC3     object\nDMDEDUC2     object\nDMDSCHOL     object\nDMDMARTL      int64\nDMDHHSIZ      int64\nDMDFMSIZ      int64\nINDHHINC     object\nINDFMINC     object\nINDFMPIR     object\nRIDEXPRG     object\nDMDHRGND      int64\nDMDHRAGE      int64\nDMDHRBRN     object\nDMDHREDU     object\nDMDHRMAR     object\nDMDHSEDU     object\nSIALANG      object\nSIAPROXY     object\nSIAINTRP     object\nFIALANG      object\nFIAPROXY     object\nFIAINTRP     object\nMIALANG      object\nMIAPROXY     object\nMIAINTRP     object\nAIALANG      object\nWTINT2YR    float64\nWTMEC2YR    float64\nSDMVPSU       int64\nSDMVSTRA      int64\ndtype: object\n\n\nWe need to turn some columns into integer\n\nimport numpy as np\nDS209 = DS209.replace(\" \", np.NaN )\n\n\nDS209[DS209.columns[1:11]]\nconvert_dict = {}\nfor column in DS209.columns[1:11]:\n    DS209[column] = pd.to_numeric(DS209[column], errors='coerce')\nprint(DS209.dtypes)\n\nSEQN          int64\nDPQ010      float64\nDPQ020      float64\nDPQ030      float64\nDPQ040      float64\nDPQ050      float64\nDPQ060      float64\nDPQ070      float64\nDPQ080      float64\nDPQ090      float64\nDPQ100      float64\nSDDSRVYR      int64\nRIDSTATR      int64\nRIDEXMON      int64\nRIAGENDR      int64\nRIDAGEYR      int64\nRIDAGEMN     object\nRIDAGEEX     object\nRIDRETH1      int64\nDMQMILIT      int64\nDMDBORN       int64\nDMDCITZN      int64\nDMDYRSUS     object\nDMDEDUC3     object\nDMDEDUC2     object\nDMDSCHOL     object\nDMDMARTL      int64\nDMDHHSIZ      int64\nDMDFMSIZ      int64\nINDHHINC     object\nINDFMINC     object\nINDFMPIR     object\nRIDEXPRG     object\nDMDHRGND      int64\nDMDHRAGE      int64\nDMDHRBRN     object\nDMDHREDU     object\nDMDHRMAR     object\nDMDHSEDU     object\nSIALANG      object\nSIAPROXY     object\nSIAINTRP     object\nFIALANG      object\nFIAPROXY     object\nFIAINTRP     object\nMIALANG      object\nMIAPROXY     object\nMIAINTRP     object\nAIALANG      object\nWTINT2YR    float64\nWTMEC2YR    float64\nSDMVPSU       int64\nSDMVSTRA      int64\ndtype: object\n\n\n\n(DS209.iloc[:, 1:11].isna().mean(axis = 1) == 1).sum()\n\n498\n\n\n498 participants did not respond at all. Let’s remove these from the dataset\n\nclean = DS209[DS209.iloc[:, 1:11].isnull().mean(axis = 1) != 1]\n\n\ndepression_score = clean.iloc[:, 1:11].mean(axis = 1)\n\n\nfinal = pd.concat([clean, depression_score], axis = 1).rename(columns = {0:\"DepressionScore\"}).set_index(\"SEQN\")",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#distribution-of-target-variable",
    "href": "Applications/Application1/EDA.html#distribution-of-target-variable",
    "title": "2  Data Pre-Processing",
    "section": "2.8 Distribution of target variable",
    "text": "2.8 Distribution of target variable\n\nfinal[\"DepressionScore\"].describe()\n\ncount    4836.000000\nmean        0.306493\nstd         0.437908\nmin         0.000000\n25%         0.000000\n50%         0.100000\n75%         0.400000\nmax         4.900000\nName: DepressionScore, dtype: float64\n\n\n\nimport matplotlib.pyplot as plt\nfrom math import log\n\nplt.hist(final[\"DepressionScore\"], bins = 15)\nplt.title(\"Distribution of Depression Scores\")\nplt.xlabel(\"Depression Score\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfinal.to_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"/Data/NHANES/depression_table.csv\")",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Notebooks/boosting.html",
    "href": "Applications/Notebooks/boosting.html",
    "title": "3  code adapted from https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function",
    "section": "",
    "text": "3.1 Boosting\nif(!require(\"adabag\")){install.packages(\"adabag\")}\nlibrary(adabag)\n#model_boost &lt;- boosting(DepressionScore~.,data=train)",
    "crumbs": [
      "Post-Midterm",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>code adapted from https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function</span>"
    ]
  }
]