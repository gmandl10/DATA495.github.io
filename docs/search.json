[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "1 Introduction",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R1.html",
    "href": "Weekly_Reports/R1.html",
    "title": "2  DATA495 Weekly Report 1",
    "section": "",
    "text": "3 Weekly Report 1\nDate: January 19, 2024",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DATA495 Weekly Report 1</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R1.html#past-objective-status",
    "href": "Weekly_Reports/R1.html#past-objective-status",
    "title": "2  DATA495 Weekly Report 1",
    "section": "3.1 Past Objective Status",
    "text": "3.1 Past Objective Status\nN/A",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DATA495 Weekly Report 1</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R1.html#weekly-work-log",
    "href": "Weekly_Reports/R1.html#weekly-work-log",
    "title": "2  DATA495 Weekly Report 1",
    "section": "3.2 Weekly Work Log",
    "text": "3.2 Weekly Work Log\n\n\n\n\n\n\n\n\nActivity\nDescription\nTime Allocated\n\n\n\n\nApplication 1\nSearch for dataset, data preprocessing in Python, exploratory data analysis\n2 hours\n\n\nElements of Statistical Learning\n9.1 Generalized Additive Models, 9.4 Multivariate Adaptive Regression Splines\n2 hours",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DATA495 Weekly Report 1</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R1.html#upcoming-objectives",
    "href": "Weekly_Reports/R1.html#upcoming-objectives",
    "title": "2  DATA495 Weekly Report 1",
    "section": "3.3 Upcoming Objectives",
    "text": "3.3 Upcoming Objectives\n\n\n\n\n\n\n\nObjective\nDescription\n\n\n\n\nLiterature Review\nExplore explainability of GAMs and spline based methods, application of machine learning in precision medicine (specifically GAMs and splines)\n\n\nElements of Statistical Learning\nRead Chapter 10: Boosting and Additive Trees\n\n\nApplication 1\nPrepare elementary statistical models which we can improve on with the next application assignments",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DATA495 Weekly Report 1</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R2.html",
    "href": "Weekly_Reports/R2.html",
    "title": "3  DATA495 Weekly Report 2",
    "section": "",
    "text": "4 Weekly Report 2\nDate: January 26, 2024",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DATA495 Weekly Report 2</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R2.html#past-objective-status",
    "href": "Weekly_Reports/R2.html#past-objective-status",
    "title": "3  DATA495 Weekly Report 2",
    "section": "4.1 Past Objective Status",
    "text": "4.1 Past Objective Status\n\n\n\n\n\n\n\n\nObjective\nDescription\nStatus\n\n\n\n\nLiterature Review\nExplore explainability of GAMs and spline based methods, application of machine learning in precision medicine (specifically GAMs and splines)\nComplete, see below\n\n\nElements of Statistical Learning\nRead Chapter 10: Boosting and Additive Trees\nPartially Complete\n\n\nApplication 1\nPrepare elementary statistical models which we can improve on with the next application assignments\nComplete",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DATA495 Weekly Report 2</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R2.html#weekly-work-log",
    "href": "Weekly_Reports/R2.html#weekly-work-log",
    "title": "3  DATA495 Weekly Report 2",
    "section": "4.2 Weekly Work Log",
    "text": "4.2 Weekly Work Log\n\n\n\nActivity\nDescription\nTime Allocated\n\n\n\n\nApplication 1\nExploratory Data Analysis, Preliminary Modeling\n2 hours\n\n\nElements of Statistical Learning\nChapter 10.1-10.4\n1 hour\n\n\nLiterature Review\nSee below\n1 hour",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DATA495 Weekly Report 2</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R2.html#upcoming-objectives",
    "href": "Weekly_Reports/R2.html#upcoming-objectives",
    "title": "3  DATA495 Weekly Report 2",
    "section": "4.3 Upcoming Objectives",
    "text": "4.3 Upcoming Objectives\n\n\n\n\n\n\n\nObjective\nDescription\n\n\n\n\nLiterature Review\nExplore explain ability boosting, application of machine learning in NHANES and precision medicine (specifically GAMs and boosting)\n\n\nElements of Statistical Learning\nFinish reading Chapter 10: Boosting and Additive Trees\n\n\nApplication 2\nPrepare applications of GAMs to NHANES data",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DATA495 Weekly Report 2</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R2.html#literature-review",
    "href": "Weekly_Reports/R2.html#literature-review",
    "title": "3  DATA495 Weekly Report 2",
    "section": "4.4 Literature Review",
    "text": "4.4 Literature Review\n\n\n\n\n\n\n\n\nTitle\nISSN\nComments\n\n\n\n\nMachine Learning Model for Predicting CVD Risk on NHANES Data (search: machine learning NHANEs)\n2694-0604\nclassify NHANES participants into 3 categories: Healthy, non-healthy, needs further tests.used SVM with radial kernel as classifier.tested independently on 4 different ages groups.Let’s reproduce this with different types of machine learning models and try to analyze interpretability of our classifications\n\n\nGAMI-Net: An explainable neural network based on generalized additive models with structured interactions (search: explainability generalized additive models)\n0031-3203\ncontends to balance trade-off of prediction accuracy (from neural networks) and model interpretability (from GAMs) interpretability aspects: sparsity - only including the most important effects, heredity - interactions only include if parent terms are already present in the model, marginal clarity - “to make main effects and pairwise interactions mutually distinguishable” (not sure what this is supposed to mean) reduces the redundancy of the main terms and their presence in interactions.training of the model occurs in two steps: first model is trained on main effects and then interaction terms are fit to residuals.GAMI-net allows you to get local interpretations for individual observations.*Can calculate the importance ratio (IR) for each effect and use it to determine how important a variable was in 1. the entire model 2. in an individual observation. The higher the IR, the more important",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DATA495 Weekly Report 2</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R3.html",
    "href": "Weekly_Reports/R3.html",
    "title": "4  DATA495 Weekly Report 3",
    "section": "",
    "text": "5 Weekly Report 3\nDate: February 2nd, 2024",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DATA495 Weekly Report 3</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R3.html#past-objective-status",
    "href": "Weekly_Reports/R3.html#past-objective-status",
    "title": "4  DATA495 Weekly Report 3",
    "section": "5.1 Past Objective Status",
    "text": "5.1 Past Objective Status\n\n\n\n\n\n\n\n\nObjective\nDescription\nStatus\n\n\n\n\nLiterature Review\nExplore explain ability boosting, application of machine learning in NHANES and precision medicine (specifically GAMs and boosting)\nComplete, see below\n\n\nElements of Statistical Learning\nRead Chapter 10: Boosting and Additive Trees\nPartially Complete\n\n\nApplication 1\nPrepare elementary statistical models which we can improve on with the next application assignments\nComplete",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DATA495 Weekly Report 3</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R3.html#weekly-work-log",
    "href": "Weekly_Reports/R3.html#weekly-work-log",
    "title": "4  DATA495 Weekly Report 3",
    "section": "5.2 Weekly Work Log",
    "text": "5.2 Weekly Work Log\n\n\n\nActivity\nDescription\nTime Allocated\n\n\n\n\nApplication 2\nNew dataset loading, boosting and mars\n2 hours\n\n\nElements of Statistical Learning\nChapter 10 Boosting\n1 hour\n\n\nLiterature Review\nSee below\n1.5 hour\n\n\nGitHub setup\n/\n1 hour",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DATA495 Weekly Report 3</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R3.html#upcoming-objectives",
    "href": "Weekly_Reports/R3.html#upcoming-objectives",
    "title": "4  DATA495 Weekly Report 3",
    "section": "5.3 Upcoming Objectives",
    "text": "5.3 Upcoming Objectives\n\n\n\n\n\n\n\nObjective\nDescription\n\n\n\n\nLiterature Review\nExplainable/Trustworthy AI in Medicine - what are the requirements and how can we build models that meet these requirements\n\n\nElements of Statistical Learning\nDiscuss topics with Devan in weekly meeting\n\n\nApplication 2\nDue Monday night, complete analysis of boosting and mars models.",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DATA495 Weekly Report 3</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R3.html#literature-review",
    "href": "Weekly_Reports/R3.html#literature-review",
    "title": "4  DATA495 Weekly Report 3",
    "section": "5.4 Literature Review",
    "text": "5.4 Literature Review\n\n\n\n\n\n\n\n\nTitle\nISSN\nComments\n\n\n\n\nMining the breast cancer pattern using artificial neural networks and multivariate adaptive regression splines\n0957-4174\nIn this paper, the authors suggest using MARS for determining important variables and then only using these variables as input in a neural network. This neural network, although more complex, performs worse than the MARS model on out of sample classification of breast cancer patients. Maybe interactions are significant in this data and MARS is able to pick these up better than an ANN.\n\n\nMultivariate adaptive regression splines analysis to predict biomarkers of spontaneous preterm birth\n1600-0412\nFocus of this analysis was not so much on importance of variables but the presence of certain variables in models for Caucasians and for African Americans. They looked at a MARS model for all the data and the basis functions and then the same for both races. Interesting is that the model for African Americans has a significantly better AUC than the model for Caucasians although the sample size for both categories is approximately the same\n\n\nAdaptive splines-based logistic regression with a ReLU neural network\nhttps://hal.science/hal-03778323\nThe authors tried to combine the neural network structure but at each neuron they looked at the interaction between 2 knots. The formula for the model looks very similar to the MARS formula. Inside the activation function of the suggested NN-MARS model we add various components from the collection of basis functions C, whereas in MARS the activation function contains the products of functions in C. The results are nothing special: a regular neural network outpreforms the NN-MARS and quite similarily to a MARS model in both accuracy and AUC. Also the fitting of the NN-MARS model takes longer on average than a NN.",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DATA495 Weekly Report 3</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R4.html",
    "href": "Weekly_Reports/R4.html",
    "title": "5  DATA495 Weekly Report 4",
    "section": "",
    "text": "6 Weekly Report 4\nDate: February 9th, 2024",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DATA495 Weekly Report 4</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R4.html#past-objective-status",
    "href": "Weekly_Reports/R4.html#past-objective-status",
    "title": "5  DATA495 Weekly Report 4",
    "section": "6.1 Past Objective Status",
    "text": "6.1 Past Objective Status\n\n\n\n\n\n\n\n\nObjective\nDescription\nStatus\n\n\n\n\nLiterature Review\nExplainable/Trustworthy AI in Medicine - what are the requirements and how can we build models that meet these requirements\nComplete, see below\n\n\nElements of Statistical Learning\nDiscuss topics with Devan in weekly meeting\n\n\n\nApplication 2\nDue Monday night, complete analysis of boosting and mars models.",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DATA495 Weekly Report 4</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R4.html#weekly-work-log",
    "href": "Weekly_Reports/R4.html#weekly-work-log",
    "title": "5  DATA495 Weekly Report 4",
    "section": "6.2 Weekly Work Log",
    "text": "6.2 Weekly Work Log\n\n\n\n\n\n\n\n\nActivity\nDescription\nTime Allocated\n\n\n\n\nApplication 2\nBoosting and zero inflated models\n3 hours\n\n\nElements of Statistical Learning\nChapter 9.2 Tree-Based Methods\n45 minutes\n\n\nLiterature Review\nSee below\n1 hour",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DATA495 Weekly Report 4</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R4.html#upcoming-objectives",
    "href": "Weekly_Reports/R4.html#upcoming-objectives",
    "title": "5  DATA495 Weekly Report 4",
    "section": "6.3 Upcoming Objectives",
    "text": "6.3 Upcoming Objectives\n\n\n\nObjective\nDescription\n\n\n\n\nLiterature Review\nExplainable tree based methods\n\n\nElements of Statistical Learning\n9.3 Prim Bump Hunting\n\n\nApplication 3\nImplement explainable trees\n\n\nMidterm Knowledge Report\nPrepare infographics",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DATA495 Weekly Report 4</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R4.html#literature-review",
    "href": "Weekly_Reports/R4.html#literature-review",
    "title": "5  DATA495 Weekly Report 4",
    "section": "6.4 Literature Review",
    "text": "6.4 Literature Review\n\n\n\n\n\n\n\n\nTitle\nLink\nComments\n\n\n\n\nImproving the explainability of Random Forest classifier – user centered approach\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728671/\nThis article is extremely relevant to this course and my future interests in ML in the health care industry. It outlines the process of creating explainable machine learning tools for use by industry professionals. The authors began their work with discussions with practitioners regarding their expectations for relying on AI in their work. The authors created a one-page graphic that addressing all the expectations and enhances the decisions made by practitioners. I really like this article because it provides a framework for how we can incorporate AI into the bio sciences.\n\n\nPrediction of slaughter age in pigs and assessment of the predictive value of phenotypic and genetic information using random forest\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6276566/\nI found this article because it was cited the previous article and I wanted to find further extensions of explainable interfaces in the context of biosciences, specifically genetics. These authors attempted to predict pig slaughter age using phenotype factors and although successful, did not explicitly create a explainability interface. They included a graph of the feature importances but this does not solve the problem of making machine learning results understandable for non-expert users from the industry. They only referenced the previous article in the discussion as a framework for enhancing explainability.",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DATA495 Weekly Report 4</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R5.html",
    "href": "Weekly_Reports/R5.html",
    "title": "6  DATA495 Weekly Report 5",
    "section": "",
    "text": "7 Weekly Report 5\nDate: February 16th, 2024",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>DATA495 Weekly Report 5</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R5.html#past-objective-status",
    "href": "Weekly_Reports/R5.html#past-objective-status",
    "title": "6  DATA495 Weekly Report 5",
    "section": "7.1 Past Objective Status",
    "text": "7.1 Past Objective Status\n\n\n\n\n\n\n\n\nObjective\nDescription\nStatus\n\n\n\n\nLiterature Review\nExplainable tree-based methods\nIncomplete, no new articles analyzed. Reviewed explainability of random forest classifier paper.\n\n\nElements of Statistical Learning\n9.3 Prim\nComplete\n\n\nApplication 3\nImplement explainable trees\nIncomplete - focus was on Midterm Assignments\n\n\nMidterm Knowledge Exchange\nPrepare infographics\nComplete",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>DATA495 Weekly Report 5</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R5.html#weekly-work-log",
    "href": "Weekly_Reports/R5.html#weekly-work-log",
    "title": "6  DATA495 Weekly Report 5",
    "section": "7.2 Weekly Work Log",
    "text": "7.2 Weekly Work Log\n\n\n\n\n\n\n\n\nActivity\nDescription\nTime Allocated\n\n\n\n\nMidterm Knowledge Exchange\nCreating visualizations for infographics and building models to use in examples\n4.5 hours\n\n\nElements of Statistical Learning\nChapter 9.3 Prim\n30 minutes\n\n\nIntepretML\nExplainable Boosting Machines and API use\n0.5 hour",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>DATA495 Weekly Report 5</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R5.html#upcoming-objectives",
    "href": "Weekly_Reports/R5.html#upcoming-objectives",
    "title": "6  DATA495 Weekly Report 5",
    "section": "7.3 Upcoming Objectives",
    "text": "7.3 Upcoming Objectives\n\n\n\nObjective\nDescription\n\n\n\n\nLiterature Review\nRequirements for explainable AI in health care\n\n\nIntepretML\nLook further into use of API and libraries\n\n\nApplication 3\nImplement explainable trees",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>DATA495 Weekly Report 5</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R7.html",
    "href": "Weekly_Reports/R7.html",
    "title": "7  DATA495 Weekly Report 7",
    "section": "",
    "text": "8 Weekly Report 7\nDate: March 8th, 2024",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>DATA495 Weekly Report 7</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R7.html#past-objective-status",
    "href": "Weekly_Reports/R7.html#past-objective-status",
    "title": "7  DATA495 Weekly Report 7",
    "section": "8.1 Past Objective Status",
    "text": "8.1 Past Objective Status\n\n\n\n\n\n\n\n\nObjective\nDescription\nStatus\n\n\n\n\nLiterature Review\nRequirements for explainable AI in health care\nIncomplete\n\n\nIntepretML\nLook further into use of API and libraries\nComplete\n\n\nApplication 3\nImplement explainable trees\nComplete",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>DATA495 Weekly Report 7</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R7.html#weekly-work-log",
    "href": "Weekly_Reports/R7.html#weekly-work-log",
    "title": "7  DATA495 Weekly Report 7",
    "section": "8.2 Weekly Work Log",
    "text": "8.2 Weekly Work Log\n\n\n\n\n\n\n\n\nActivity\nDescription\nTime Allocated\n\n\n\n\nReading\nSHAP, LIME, shapley values\n2 hours\n\n\nApplication 4\nClean up past applications and apply LIME and SHAP\n3 hours",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>DATA495 Weekly Report 7</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R7.html#upcoming-objectives",
    "href": "Weekly_Reports/R7.html#upcoming-objectives",
    "title": "7  DATA495 Weekly Report 7",
    "section": "8.3 Upcoming Objectives",
    "text": "8.3 Upcoming Objectives\n\n\n\n\n\n\n\nObjective\nDescription\n\n\n\n\nLiterature Review\nSHAP and LIME applications in health care and use in compliance\n\n\nReading\nNeural Nets?\n\n\nApplication 5\nImplement Neural Nets with LIME and SHAP",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>DATA495 Weekly Report 7</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R8.html",
    "href": "Weekly_Reports/R8.html",
    "title": "8  DATA495 Weekly Report 8",
    "section": "",
    "text": "9 Weekly Report 8\nDate: March 15th, 2024",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DATA495 Weekly Report 8</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R8.html#past-objective-status",
    "href": "Weekly_Reports/R8.html#past-objective-status",
    "title": "8  DATA495 Weekly Report 8",
    "section": "9.1 Past Objective Status",
    "text": "9.1 Past Objective Status\n\n\n\n\n\n\n\nObjective\nDescription\n\n\n\n\nLiterature Review\nSHAP and LIME applications in health care and use in compliance\n\n\nApplication 4\nLime and SHAP\n\n\nApplication 5\nImplement Neural Nets with LIME and SHAP",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DATA495 Weekly Report 8</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R8.html#weekly-work-log",
    "href": "Weekly_Reports/R8.html#weekly-work-log",
    "title": "8  DATA495 Weekly Report 8",
    "section": "9.2 Weekly Work Log",
    "text": "9.2 Weekly Work Log\n\n\n\nActivity\nDescription\nTime Allocated\n\n\n\n\nLiterature Review\nSee below\n2 hours\n\n\nApplication 4\nLIME and SHAP\n3 hours",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DATA495 Weekly Report 8</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R8.html#upcoming-objectives",
    "href": "Weekly_Reports/R8.html#upcoming-objectives",
    "title": "8  DATA495 Weekly Report 8",
    "section": "9.3 Upcoming Objectives",
    "text": "9.3 Upcoming Objectives\n\n\n\n\n\n\n\nObjective\nDescription\n\n\n\n\nLiterature Review\nMore SHAP and LIME (maybe as they pertain to neural nets)\n\n\nApplication 5\nImplement Neural Nets with LIME and SHAP",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DATA495 Weekly Report 8</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R8.html#literature-review",
    "href": "Weekly_Reports/R8.html#literature-review",
    "title": "8  DATA495 Weekly Report 8",
    "section": "9.4 Literature Review",
    "text": "9.4 Literature Review\n\nComparative analysis of explainable machine learning prediction models for hospital mortality\nhttps://doi.org/10.1186/s12874-022-01540-w This paper attempts to create machine learning models that predict an ICU patient’s survival. Several models are trained and SHAP values are fit for each and compared across models to show the discrepancies leading to predictions in different models. The authors also demonstrate how SHAP is used in the health care field. An example is using the force plot of a single observation to say for example: the patient’s age is a factor pushing the predictions towards survival or the patient’s temperature has the most significant impact on our prediction. All in all, it is a good demonstration of a positive use of explainable AI in health care. I would like to review other papers that have a more critical perspective on XAI in health care. ### Explainable AI and Interpretable Machine Learning: A Case Study in Perspective https://www.sciencedirect.com/science/article/pii/S1877050922008432 Poor quality -&gt; didn’t even have a designated results section and the images where screenshots of code from notebook ### Generating Counterfactual and Contrastive Explanations using SHAP http://arxiv.org/abs/1906.09293 This paper is written to address the General Data Protection Regulations that have been created to address data protection in high-risk use cases. The specific focus is on “the right to explanation”. Under some circumstances, the user that is being affected by the decisions of the model has the right to know how the model is coming to its decision (A good example is somebody who is applying for a loan). This is significant for two reasons: 1. Data Scientists must now extend their models to include explanations that comply with the GDPR and 2. the inner workings of a proprietary model will be exposed. In the paper, the authors outline the characteristics of an explanation and how the explanation must be formatted in order to comply with the right to explanation. Counterfactual explanations are significant and are described as “the minimum possible change required to generate the desired output” (Rathi, 2019, p. 2) (pdf). The authors describe a loss function that takes the instance of interest as well as a counterfactual outcome and measures how much the input is changed for it to result in the counterfactual outcome (THIS IS SUPER INTERESTING IMO, feels like this will lead to some new statistical domain of explainability).To answer the question why P not Q (eg why am I declined and not accepted for this loan) the shap values determined for classes P and Q. The positive effects for P are given and the negative effects for Q are given. This way the user can understand what factors are leading to P and ‘steering them away’ from Q. For counterfactual explanations, the nearest neighbors of an instance are generated until a counterfactual is produced. ### The Disagreement Problem in Explainable Machine Learning: A Practitioner’s Perspective http://arxiv.org/abs/2202.01602 This authors of this paper highlight a significant drawbacks of explain-ability in AI and that is when the metrics for explanation degree with one another. The authors conducted a survey on 25 data scientists to understand the explain-ability preferences of practitioners, how frequently they face disagreement, and what their course of action is when models disagree. Further, the paper also theorizes metrics that be used to evaluate the disagreement between machine learning models. Overall, I think these disagreement metrics could be a useful tool if standards of use can be agreed upon. (This is just another sub-field of the new explainbility domain of statistics lol)",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DATA495 Weekly Report 8</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html",
    "href": "Applications/Application1/EDA.html",
    "title": "9  Data Pre-Processing",
    "section": "",
    "text": "9.1 DS0215\nDietary Supplement Use – Ingredient Information\nThe Supplement ID number (DSDSUPID) is also the primary key in DS0214 and is a foreign key in DS0213\nCan a supplement have several ingredients and what is the distributions of number ingredients in a supplements\nDS2015 = pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd())) + \"\\\\Data\\\\NHANES\\\\ICPSR_25504\\\\DS0215\\\\25504-0215-Data.tsv\", sep = \"\\t\")\nnum_ingredients = DS2015.groupby(\"DSDSUPP\").size().sort_values(ascending = False).reset_index(name=\"Count\")\nplt.hist(num_ingredients[\"Count\"], bins = 12)\nplt.xlabel(\"Number of Ingredients in Supplement\")\nplt.ylabel(\"Number of Supplements\")\nplt.title(\"Distribution of Amount of Ingredients Among Supplements\")\nplt.show()",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#ds0216",
    "href": "Applications/Application1/EDA.html#ds0216",
    "title": "9  Data Pre-Processing",
    "section": "9.2 DS0216",
    "text": "9.2 DS0216\nDietary Supplement Use – Supplement Blend\nThe primary key in DS0216 is the Ingredient ID number which is a foreign key in DS0215",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#ds0235",
    "href": "Applications/Application1/EDA.html#ds0235",
    "title": "9  Data Pre-Processing",
    "section": "9.3 DS0235",
    "text": "9.3 DS0235\nDrug Information\nThe primary key for DS0235 is the Generic Drug Code (RXDDRGID). Let’s find which dataset has the generic drug code as a foreign key.\n\nfor i in dataframes.index:\n    df = dataframes[i]\n    \n    if \"RXDDRGID\" in df.columns:\n        print(i)\n\nDS0234\nDS0235\n\n\nDS0234 contains RXDDRGID as a foreign key. This dataset is Prescription Medications\n\nDS2034 = pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd())) + \"\\\\Data\\\\NHANES\\\\ICPSR_25504\\\\DS0234\\\\25504-0234-Data.tsv\", sep = \"\\t\")\nnum_drugs = DS2034.groupby(\"SEQN\").size().sort_values(ascending = False).reset_index(name=\"Count\")\n\n\n# Partipicant 41025 uses 20 perscription drugs\nDS2034[DS2034[\"SEQN\"] == 41025]\n\n\n\n\n\n\n\n\n\nSEQN\nRXDUSE\nRXDDRUG\nRXDDRGID\nRXQSEEN\nRXDDAYS\nRXDCOUNT\nSDDSRVYR\nRIDSTATR\nRIDEXMON\n...\nFIAPROXY\nFIAINTRP\nMIALANG\nMIAPROXY\nMIAINTRP\nAIALANG\nWTINT2YR\nWTMEC2YR\nSDMVPSU\nSDMVSTRA\n\n\n\n\n17125\n41025\n1\nACETAMINOPHEN; HYDROCODONE\nd03428\n1\n1825\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17126\n41025\n1\nAZELASTINE NASAL\nd04068\n1\n182\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17127\n41025\n1\nBUSPIRONE\nd00182\n1\n122\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17128\n41025\n1\nCHOLESTYRAMINE\nd00193\n1\n547\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17129\n41025\n1\nCITALOPRAM\nd04332\n1\n243\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17130\n41025\n1\nDILTIAZEM\nd00045\n1\n365\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17131\n41025\n1\nESOMEPRAZOLE\nd04749\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17132\n41025\n1\nFLUTICASONE\nd01296\n1\n1825\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17133\n41025\n1\nFLUTICASONE; SALMETEROL\nd04611\n1\n1095\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17134\n41025\n1\nGABAPENTIN\nd03182\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17135\n41025\n1\nGUAIFENESIN; PSEUDOEPHEDRINE\nd03379\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17136\n41025\n1\nLORAZEPAM\nd00149\n1\n365\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17137\n41025\n1\nMETAXALONE\nd00964\n1\n1460\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17138\n41025\n1\nNABUMETONE\nd00310\n1\n243\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17139\n41025\n1\nOXYBUTYNIN\nd00328\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17140\n41025\n1\nOXYCODONE\nd00329\n1\n1825\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17141\n41025\n1\nQUININE\nd00366\n1\n10950\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17142\n41025\n1\nSUMATRIPTAN\nd03160\n1\n1825\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17143\n41025\n1\nTAMSULOSIN\nd04121\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17144\n41025\n1\nTRAZODONE\nd00395\n1\n365\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n\n\n20 rows × 49 columns\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.hist(num_drugs[\"Count\"], bins = 10)\nplt.xlabel(\"Number of Perscription Drugs Used\")\nplt.ylabel(\"Number of Participants\")\nplt.title(\"Perpsection Drug Usage Among Study Participants\")\nplt.show()\n\n\n\n\n\n\n\n\nNote: Like with supplement usage, the patients who do not use prescription medications are not represented in this visualization",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#year-to-year-survey-data-consistency",
    "href": "Applications/Application1/EDA.html#year-to-year-survey-data-consistency",
    "title": "9  Data Pre-Processing",
    "section": "9.4 Year to Year Survey Data Consistency",
    "text": "9.4 Year to Year Survey Data Consistency\nThe data that we have been working with from the dataset ICPSR_25504 is the National Health and Nutrition Examination Survey (NHANES) from 2005-2006. Next we have downloaded the Survey from 2005-2006 and we will look to see if the data architecture is consistent across year.\nFirst let’s check if each data set exists in both surveys and if their columns are the same\n\nimport os\nimport pandas as pd\n\ndata_master = os.path.dirname(os.path.dirname(os.getcwd())) + \"\\\\Data\\\\NHANES\\\\ICPSR_25505\\\\\"\ndataframes2 = []\nindex2 = []\ni = 0\n\nos.listdir(data_master)\nfor root, dirs, files in os.walk(data_master):\n    for name in files:\n        if name.endswith(\".tsv\"):\n            file = os.path.join(root, name)\n            dataframes2.append(pd.read_csv(file, sep = \"\\t\", nrows = 100))\n            index2.append(root[-6:])\n\ndataframes2 = pd.Series(dataframes2, index = index2)\n\n\nprint(\"Datasets included in 2005-2006 but not in 2007-2008\")\nfor i in index:\n    if i not in index2:\n        print(i)\n\n\nprint(\"Datasets not included in 2005-2006 but in 2007-2008\")\nfor i in index2:\n    if i not in index:\n        print(i)\n\nprint(\"Datasets with mismatching columns\")\nfor i in index:\n    if i in index2:\n        if len(dataframes[i].columns) != len(dataframes2[i].columns):\n            print(i)\n        \nprint(\"Datasets with consistent number of columns\")\nfor i in index:\n    if i in index2:\n        if len(dataframes[i].columns) == len(dataframes2[i].columns):\n            print(i)\n        \n\nDatasets included in 2005-2006 but not in 2007-2008\nDS0018\nDS0019\nDS0020\nDS0021\nDS0022\nDS0023\nDS0024\nDS0025\nDS0026\nDS0128\nDS0129\nDS0130\nDS0131\nDS0132\nDS0133\nDS0134\nDS0135\nDS0136\nDS0137\nDS0138\nDS0139\nDS0140\nDS0141\nDS0142\nDS0143\nDS0144\nDS0234\nDS0235\nDS0236\nDS0237\nDS0238\nDS0239\nDS0240\nDS0241\nDS0242\nDS0243\nDS0244\nDS0245\nDS0246\nDS0247\nDS0248\nDatasets not included in 2005-2006 but in 2007-2008\nDS0100\nDatasets with mismatching columns\nDS0011\nDS0012\nDS0013\nDS0014\nDS0015\nDS0016\nDS0017\nDS0101\nDS0102\nDS0103\nDS0104\nDS0105\nDS0106\nDS0107\nDS0108\nDS0109\nDS0110\nDS0111\nDS0113\nDS0114\nDS0115\nDS0116\nDS0117\nDS0118\nDS0119\nDS0120\nDS0121\nDS0122\nDS0123\nDS0124\nDS0125\nDS0126\nDS0127\nDS0203\nDS0204\nDS0206\nDS0207\nDS0208\nDS0209\nDS0210\nDS0211\nDS0212\nDS0214\nDS0215\nDS0216\nDS0217\nDS0218\nDS0219\nDS0220\nDS0221\nDS0222\nDS0223\nDS0224\nDS0225\nDS0226\nDS0227\nDS0228\nDS0229\nDS0230\nDS0231\nDS0232\nDS0233\nDatasets with consistent number of columns\nDS0001\nDS0112\nDS0201\nDS0202\nDS0205\nDS0213\n\n\nLet’s see how many participants we have from both surveys\n\nICPSR_25504 = pd.read_csv(\"ICPSR_25504\\\\DS0001\\\\25504-0001-Data.tsv\", sep = \"\\t\")\nICPSR_25505 = pd.read_csv(\"ICPSR_25505\\\\DS0001\\\\25505-0001-Data.tsv\", sep = \"\\t\")\nICPSR_25504[\"SEQN\"].apply(lambda x: x in ICPSR_25505[\"SEQN\"].values).sum()\n\n0\n\n\nThere is no way of identifying previous participants in new surverys.\nThere seems to be little value in analyzing multiple surveys because we can’t create variables that look at year to year changes in the same patient and the data architecture is not consistent across years.\nThe only potential uses of having surveys from different years is to see if a model built on data from one year generalizes to the survey data from other years. In order to do some we must choose variables and datasets that exist in all surveys.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#joining-datasets",
    "href": "Applications/Application1/EDA.html#joining-datasets",
    "title": "9  Data Pre-Processing",
    "section": "9.5 Joining Datasets",
    "text": "9.5 Joining Datasets\n\nimport pandas as pd\nDS0132= pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd())) + \"\\\\Data\\\\NHANES\\\\ICPSR_25504\\\\DS0132\\\\25504-0132-Data.tsv\", sep = \"\\t\")\nDS0123 = pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd())) + \"\\\\Data\\\\NHANES\\\\ICPSR_25504\\\\DS0123\\\\25504-0123-Data.tsv\", sep = \"\\t\")\njoined = DS0132.set_index(\"SEQN\").join(DS0123.set_index(\"SEQN\"), lsuffix=\"_DS0132\", rsuffix=\"_DS0123\")\n\n\njoined.head()\n\n\n\n\n\n\n\n\n\nWTSAF2YR_DS0132\nLBXTR\nLBDTRSI\nLBDLDL\nLBDLDLSI\nLBXAPB\nLBDAPBSI\nSDDSRVYR_DS0132\nRIDSTATR_DS0132\nRIDEXMON_DS0132\n...\nFIAPROXY_DS0123\nFIAINTRP_DS0123\nMIALANG_DS0123\nMIAPROXY_DS0123\nMIAINTRP_DS0123\nAIALANG_DS0123\nWTINT2YR_DS0123\nWTMEC2YR_DS0123\nSDMVPSU_DS0123\nSDMVSTRA_DS0123\n\n\nSEQN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n31130\n0.000\n\n\n\n\n\n\n4\n2\n2\n...\n2\n2\n\n\n\n\n29960.839509\n34030.994786\n2\n46\n\n\n31131\n67556.810\n86\n.971\n49\n1.267\n50\n.5\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n26457.708180\n26770.584605\n1\n48\n\n\n31132\n80193.962\n65\n.734\n75\n1.94\n75\n.75\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n32961.509920\n35315.538900\n2\n52\n\n\n31133\n15668.017\n61\n.689\n81\n2.095\n75\n.75\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n5635.221296\n5920.617679\n1\n51\n\n\n31134\n93399.539\n195\n2.202\n98\n2.534\n111\n1.11\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n43718.506372\n44231.167252\n2\n48\n\n\n\n\n5 rows × 98 columns\n\n\n\n\nLet’s see if the columns that occur in both dataframes are redundant\n\n(joined[\"AIALANG_DS0123\"] == joined[\"AIALANG_DS0132\"]).mean()\n# AIALANG is definitely redundant\n\n1.0\n\n\n\n# Compute the proportion of columns that occur in both data sets that are redundant\nDS1 = \"DS0123\"\nDS2 = \"DS0132\"\n\nn = 0\ntotal = 0\nfor column in joined.columns:\n\n    if \"_\" in column:\n        total +=1\n        original = column.split(\"_\")[0]\n        n1 = original + \"_\" + DS1\n        n2 = original + \"_\" + DS2\n\n        if (joined[n1]==joined[n2]).mean() == 1:\n            n += 1\n            joined.drop(n2, axis = 1)\n\nprint(str(n/total) + \" of the columns that occur in both datasets are redundant\")\n        \n\n1.0 of the columns that occur in both datasets are redundant\n\n\n\nimport os\nimport pandas as pd\n\ndata_master = os.path.dirname(os.path.dirname(os.getcwd())) + \"\\\\Data\\\\NHANES\\\\ICPSR_25504\\\\\"\ni = 0\njoined_master = DS0123.set_index(\"SEQN\")\n\nfor root, dirs, files in os.walk(data_master):\n    for name in files:\n        if name.endswith(\".tsv\"):\n            file = os.path.join(root, name)\n            df = pd.read_csv(file, sep = \"\\t\")\n            if \"SEQN\" not in df.columns:\n                continue\n            name = root[-6:]\n        \n            new_columns = df.columns.difference(joined_master.columns)\n            # try:\n            joined_master = joined_master.join(df.loc[:,new_columns].set_index(\"SEQN\"),how=\"left\",lsuffix=\"_master\", rsuffix=\"_\"+name)\n            # except:\n            #     print(name)\n            # n = 0\n            # total = 0\n            # for column in joined_master.columns:\n            #     if \"_\" in column:\n            #         print(column, name)\n            #         total +=1\n            #         original = column.split(\"_\")[0]\n            #         n1 = original + \"_master\"\n            #         n2 = original + \"_\" + name\n\n            #         if n2 not in joined_master.columns:\n            #             continue\n\n            #         if (joined_master[n1]==joined_master[n2]).mean() == 1:\n            #             n += 1\n            #             joined_master.drop(n2, axis = 1)\n            #             joined_master.rename({n1:original})        \n            # if n/total != 1:\n            #     print(name + \" - not redundant\")\n            \n\nKeyboardInterrupt: \n\n\n\njoined_master.head()\n\n\n\n\n\n\n\n\n\nWTSAF2YR\nLBXGLU\nLBDGLUSI\nLBXIN\nLBDINSI\nPHAFSTHR\nPHAFSTMN\nSDDSRVYR\nRIDSTATR\nRIDEXMON\n...\nFIAPROXY\nFIAINTRP\nMIALANG\nMIAPROXY\nMIAINTRP\nAIALANG\nWTINT2YR\nWTMEC2YR\nSDMVPSU\nSDMVSTRA\n\n\nSEQN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n31130\n0.000\n\n\n\n\n10\n3\n4\n2\n2\n...\n2\n2\n\n\n\n\n29960.839509\n34030.994786\n2\n46\n\n\n31131\n67556.810\n90\n4.996\n10.03\n60.18\n14\n9\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n26457.708180\n26770.584605\n1\n48\n\n\n31132\n80193.962\n157\n8.715\n8.99\n53.94\n11\n29\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n32961.509920\n35315.538900\n2\n52\n\n\n31133\n15668.017\n84\n4.663\n11.27\n67.62\n12\n32\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n5635.221296\n5920.617679\n1\n51\n\n\n31134\n93399.539\n100\n5.551\n14.51\n87.06\n14\n35\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n43718.506372\n44231.167252\n2\n48\n\n\n\n\n5 rows × 49 columns\n\n\n\n\nThe data is far too large to combine into one so I will choose critical datasets to focus the analysis on",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#ds209---depression-screener",
    "href": "Applications/Application1/EDA.html#ds209---depression-screener",
    "title": "9  Data Pre-Processing",
    "section": "9.6 DS209 - Depression Screener",
    "text": "9.6 DS209 - Depression Screener\nThis data set is a Questionnaire with 10 questions about depressive symptoms (Eg. Thought you would be better off dead.) The response are categorical from 0-3 with 0 being non-depressive behaviour (Not at all) and 4 being depressive behaviour (Nearly every day).\nWe will create a target variable that aggregate the results of these questions, taking the mean response of the questions which scores a participants depressive state. 0 meaning low, 4 high\nThe respondant can answer a question as “Don’t know” and will be assigned the number 9 which can throw off our scores. I will remove these values from the average calculations. So for example, if a respondant answers “Don’t know” for 2 questions, their score will be the average of their repsonses to the other 8 questions.\n\nDS209= pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"\\\\Data\\\\NHANES\\\\ICPSR_25504\\DS0209\\\\25504-0209-Data.tsv\", sep = \"\\t\")\n\n\nDS209.head()\n\n\n\n\n\n\n\n\n\nSEQN\nDPQ010\nDPQ020\nDPQ030\nDPQ040\nDPQ050\nDPQ060\nDPQ070\nDPQ080\nDPQ090\n...\nFIAPROXY\nFIAINTRP\nMIALANG\nMIAPROXY\nMIAINTRP\nAIALANG\nWTINT2YR\nWTMEC2YR\nSDMVPSU\nSDMVSTRA\n\n\n\n\n0\n31130\n\n\n\n\n\n\n\n\n\n...\n2\n2\n\n\n\n\n29960.839509\n34030.994786\n2\n46\n\n\n1\n31131\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n2\n1\n2\n2\n1\n26457.708180\n26770.584605\n1\n48\n\n\n2\n31132\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n2\n1\n2\n2\n1\n32961.509920\n35315.538900\n2\n52\n\n\n3\n31134\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n2\n1\n2\n2\n1\n43718.506372\n44231.167252\n2\n48\n\n\n4\n31139\n0\n0\n0\n0\n3\n1\n0\n0\n0\n...\n2\n2\n1\n2\n2\n1\n5675.705534\n5963.152246\n1\n44\n\n\n\n\n5 rows × 53 columns\n\n\n\n\n\nDS209.dtypes[\"DPQ010\"]\n\ndtype('O')\n\n\nSome of the columns are strings even though their columns contain number. We need to turn these columns into integers\n\nimport numpy as np\nDS209 = DS209.replace(\" \", np.NaN )\n\n\nDS209[DS209.columns[1:11]]\nconvert_dict = {}\nfor column in DS209.columns[1:11]:\n    DS209[column] = pd.to_numeric(DS209[column], errors='coerce')\nprint(DS209.dtypes[1:11])\n\nDPQ010    float64\nDPQ020    float64\nDPQ030    float64\nDPQ040    float64\nDPQ050    float64\nDPQ060    float64\nDPQ070    float64\nDPQ080    float64\nDPQ090    float64\nDPQ100    float64\ndtype: object\n\n\n\n(DS209.iloc[:, 1:11].isna().mean(axis = 1) == 1).sum()\n\n498\n\n\n498 participants did not respond at all. Let’s remove these from the dataset\n\nclean = DS209[DS209.iloc[:, 1:11].isnull().mean(axis = 1) != 1]\n\nI want to create one variable that summarizes the overall presence of depression in a patient (observations). I will call this the depression score and it will be the average response to the questionaire and rounding the response to the nearest integer.\n\ndepression_score = clean.iloc[:, 1:11].mean(axis = 1)\ndepression_score = round(depression_score)\n\n\nfinal = pd.concat([clean, depression_score], axis = 1).rename(columns = {0:\"DepressionScore\"}).set_index(\"SEQN\")",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#distribution-of-target-variable",
    "href": "Applications/Application1/EDA.html#distribution-of-target-variable",
    "title": "9  Data Pre-Processing",
    "section": "9.7 Distribution of target variable",
    "text": "9.7 Distribution of target variable\n\nfinal[\"DepressionScore\"].describe()\n\ncount    4836.000000\nmean        0.228908\nstd         0.507971\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%         0.000000\nmax         5.000000\nName: DepressionScore, dtype: float64\n\n\n\nfrom math import log\n\nplt.hist(final[\"DepressionScore\"], bins = 4)\nplt.title(\"Distribution of Depression Scores\")\nplt.xlabel(\"Depression Score\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfinal.to_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"/Data/NHANES/depression_table.csv\")",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/Elementary_Modeling.html",
    "href": "Applications/Application1/Elementary_Modeling.html",
    "title": "10  Initial Modeling of NHANES Data",
    "section": "",
    "text": "10.1 Loading Target Variable\nmaster = pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"/Data/NHANES/depression_table.csv\").set_index(\"SEQN\")\nmaster = pd.DataFrame(master.loc[:, \"DepressionScore\"])\nprint(\"We have the depression score for {} patients\".format(master.dropna().shape[0]))\n\nWe have the depression score for 4836 patients",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Initial Modeling of NHANES Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/Elementary_Modeling.html#loading-predictors",
    "href": "Applications/Application1/Elementary_Modeling.html#loading-predictors",
    "title": "10  Initial Modeling of NHANES Data",
    "section": "10.2 Loading Predictors",
    "text": "10.2 Loading Predictors\n\n# Blood lead and Blood Cadmium\nDS0102 = pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"/Data/NHANES/ICPSR_25504/\"+\"DS0102/25504-0102-Data.tsv\", sep = \"\\t\").set_index(\"SEQN\")\ncolumns = DS0102.columns[:np.where(DS0102.columns.values == (\"SDDSRVYR\"))[0][0]]\nmaster = master.join(DS0102[columns].replace(\" \", np.NaN).astype('float32'))\nmaster = master.replace(\" \", np.NaN )\nprint(\"We have the depression score and blood lead and cadmium levels for {} patients\".format(master.dropna().shape[0]))\n\nWe have the depression score and blood lead and cadmium levels for 4644 patients\n\n\n\n# Complete blood count\nDS0103 = pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"/Data/NHANES/ICPSR_25504/\"+\"DS0103/25504-0103-Data.tsv\", sep = \"\\t\").set_index(\"SEQN\")\ncolumns = DS0103.columns[:np.where(DS0103.columns.values == (\"SDDSRVYR\"))[0][0]]\nmaster = master.join(DS0103[columns].replace(\" \", np.NaN).astype('float32'))\nmaster = master.replace(\" \", np.NaN )\nprint(\"We have the depression score, blood counts, and blood lead and cadmium levels for {} patients\".format(master.dropna().shape[0]))\n\nWe have the depression score, blood counts, and blood lead and cadmium levels for 4614 patients\n\n\n\n# C-reactive proteins\nDS0104 = pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"/Data/NHANES/ICPSR_25504/\"+\"DS0104/25504-0104-Data.tsv\", sep = \"\\t\").set_index(\"SEQN\")\ncolumns = DS0104.columns[:np.where(DS0104.columns.values == (\"SDDSRVYR\"))[0][0]]\nmaster = master.join(DS0104[columns].replace(\" \", np.NaN).astype('float32'))\nmaster = master.replace(\" \", np.NaN )\nprint(\"We have the depression score, blood counts, blood lead and cadmium levels, and c-reactive protien levels for {} patients\".format(master.dropna().shape[0]))\n\nWe have the depression score, blood counts, blood lead and cadmium levels, and c-reactive protien levels for 4591 patients\n\n\n\n# Environmental Pesticides\nDS0105 = pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"/Data/NHANES/ICPSR_25504/\"+\"DS0105/25504-0105-Data.tsv\", sep = \"\\t\").set_index(\"SEQN\")\ncolumns = DS0105.columns[:np.where(DS0105.columns.values == (\"SDDSRVYR\"))[0][0]]\nmaster = master.join(DS0105[columns].replace(\" \", np.NaN).astype('float32'))\nmaster = master.replace(\" \", np.NaN )\nprint(\"We have the depression score, environmental pesticide levels, blood counts, blood lead and cadmium levels, and c-reactive protien levels for {} patients\".format(master.dropna().shape[0]))\n\nWe have the depression score, environmental pesticide levels, blood counts, blood lead and cadmium levels, and c-reactive protien levels for 1447 patients\n\n\nWe lose a lot of observations due to NA’s in DS0105 so we will not include the variables from DS0105 in our predictor space\n\nmaster.drop(columns, axis = 1, inplace = True)\n\n\n# Vitamin D levels\nDS0143 = pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"/Data/NHANES/ICPSR_25504/\"+\"DS0143/25504-0143-Data.tsv\", sep = \"\\t\").set_index(\"SEQN\")\ncolumns = DS0143.columns[:np.where(DS0143.columns.values == (\"SDDSRVYR\"))[0][0]]\nmaster = master.join(DS0143[columns].replace(\" \", np.NaN).astype('float32'))\nmaster = master.replace(\" \", np.NaN )\nprint(\"We have the depression score, vitamin D levels, blood counts, blood lead and cadmium levels, and c-reactive protien levels for {} patients\".format(master.dropna().shape[0]))\n\nWe have the depression score, vitamin D levels, blood counts, blood lead and cadmium levels, and c-reactive protien levels for 4591 patients",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Initial Modeling of NHANES Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/Elementary_Modeling.html#correlations",
    "href": "Applications/Application1/Elementary_Modeling.html#correlations",
    "title": "10  Initial Modeling of NHANES Data",
    "section": "10.3 Correlations",
    "text": "10.3 Correlations\n\nmaster.corr()\n\n\n\n\n\n\n\n\n\nDepressionScore\nLBXBCD\nLBDBCDSI\nLBXBPB\nLBDBPBSI\nLBXWBCSI\nLBXLYPCT\nLBXMOPCT\nLBXNEPCT\nLBXEOPCT\n...\nLBXHGB\nLBXHCT\nLBXMCVSI\nLBXMCHSI\nLBXMC\nLBXRDW\nLBXPLTSI\nLBXMPSI\nLBXCRP\nLBXVID\n\n\n\n\nDepressionScore\n1.000000\n0.082749\n0.082723\n0.035489\n0.035579\n0.082526\n-0.029590\n-0.036054\n0.041125\n-0.030285\n...\n-0.069491\n-0.064322\n-0.014322\n-0.022460\n-0.033314\n0.108660\n0.069111\n-0.004458\n0.088166\n-0.044873\n\n\nLBXBCD\n0.082749\n1.000000\n1.000000\n0.182772\n0.182755\n0.095528\n-0.004143\n-0.005835\n-0.000458\n0.012948\n...\n0.140983\n0.140536\n0.138290\n0.122018\n0.012417\n0.084576\n0.005530\n-0.021277\n0.012294\n-0.007821\n\n\nLBDBCDSI\n0.082723\n1.000000\n1.000000\n0.182733\n0.182716\n0.095535\n-0.004135\n-0.005856\n-0.000459\n0.012954\n...\n0.141013\n0.140560\n0.138275\n0.122016\n0.012446\n0.084538\n0.005544\n-0.021291\n0.012298\n-0.007837\n\n\nLBXBPB\n0.035489\n0.182772\n0.182733\n1.000000\n0.999994\n-0.049471\n-0.011938\n0.076952\n-0.012851\n0.022808\n...\n0.128587\n0.142448\n0.090765\n0.059423\n-0.045515\n0.115427\n-0.101594\n0.016290\n-0.014597\n-0.025229\n\n\nLBDBPBSI\n0.035579\n0.182755\n0.182716\n0.999994\n1.000000\n-0.049501\n-0.011853\n0.077010\n-0.012928\n0.022723\n...\n0.128571\n0.142436\n0.090806\n0.059451\n-0.045531\n0.115424\n-0.101661\n0.016333\n-0.014628\n-0.025313\n\n\nLBXWBCSI\n0.082526\n0.095528\n0.095535\n-0.049471\n-0.049501\n1.000000\n-0.264399\n-0.330581\n0.334151\n-0.093877\n...\n-0.002784\n-0.025435\n-0.029680\n0.008186\n0.093621\n0.028080\n0.232801\n0.032040\n0.226939\n0.054531\n\n\nLBXLYPCT\n-0.029590\n-0.004143\n-0.004135\n-0.011938\n-0.011853\n-0.264399\n1.000000\n0.160557\n-0.935326\n0.044943\n...\n0.058537\n0.079475\n-0.020637\n-0.046306\n-0.083245\n-0.053692\n-0.017074\n-0.019397\n-0.161871\n-0.137667\n\n\nLBXMOPCT\n-0.036054\n-0.005835\n-0.005856\n0.076952\n0.077010\n-0.330581\n0.160557\n1.000000\n-0.403857\n0.110592\n...\n0.057477\n0.066747\n0.065395\n0.044113\n-0.030895\n0.026186\n-0.164200\n-0.008881\n-0.037758\n0.017121\n\n\nLBXNEPCT\n0.041125\n-0.000458\n-0.000459\n-0.012851\n-0.012928\n0.334151\n-0.935326\n-0.403857\n1.000000\n-0.281906\n...\n-0.080062\n-0.103061\n0.003211\n0.033661\n0.089158\n0.036129\n0.044378\n0.026891\n0.156578\n0.122663\n\n\nLBXEOPCT\n-0.030285\n0.012948\n0.012954\n0.022808\n0.022723\n-0.093877\n0.044943\n0.110592\n-0.281906\n1.000000\n...\n0.088382\n0.092478\n0.007838\n0.002981\n-0.008812\n0.000926\n0.025796\n-0.035289\n-0.022729\n-0.005901\n\n\nLBXBAPCT\n0.001274\n0.055152\n0.055148\n0.011470\n0.011416\n-0.052149\n0.095774\n0.015231\n-0.160188\n0.079121\n...\n-0.036446\n-0.018897\n-0.016281\n-0.039765\n-0.078587\n0.068260\n0.053698\n-0.009205\n-0.012168\n-0.035893\n\n\nLBDLYMNO\n0.029916\n0.054873\n0.054877\n-0.025833\n-0.025767\n0.621930\n0.450831\n-0.109280\n-0.367209\n-0.032190\n...\n0.034290\n0.033536\n-0.026148\n-0.020166\n0.003894\n-0.000134\n0.117866\n-0.001059\n0.040880\n-0.044018\n\n\nLBDMONO\n0.045381\n0.078116\n0.078108\n0.027674\n0.027686\n0.540508\n-0.119009\n0.547386\n-0.029070\n0.025947\n...\n0.057163\n0.044298\n0.024091\n0.041161\n0.058614\n0.036250\n0.078929\n0.021373\n0.158769\n0.057273\n\n\nLBDNENO\n0.084803\n0.073871\n0.073879\n-0.052466\n-0.052549\n0.810719\n-0.661573\n-0.422606\n0.736214\n-0.193085\n...\n-0.039659\n-0.068083\n-0.022450\n0.021621\n0.115108\n0.033045\n0.205625\n0.041059\n0.253412\n0.097678\n\n\nLBDEONO\n0.008392\n0.059920\n0.059928\n0.011694\n0.011590\n0.218615\n-0.070112\n-0.014456\n-0.126036\n0.890855\n...\n0.090703\n0.084460\n-0.003262\n0.008246\n0.031513\n0.009769\n0.118717\n-0.018533\n0.050346\n-0.001056\n\n\nLBDBANO\n0.031953\n0.071271\n0.071277\n-0.008167\n-0.008230\n0.331551\n-0.014708\n-0.104937\n-0.012997\n0.040721\n...\n-0.038051\n-0.031110\n-0.022565\n-0.030619\n-0.034532\n0.059069\n0.123376\n-0.002560\n0.078758\n-0.010008\n\n\nLBXRBCSI\n-0.055626\n0.054540\n0.054570\n0.087274\n0.087239\n-0.010164\n0.089846\n0.029899\n-0.102944\n0.083893\n...\n0.759503\n0.826221\n-0.335899\n-0.352425\n-0.195449\n-0.080151\n-0.006242\n0.071045\n-0.090501\n-0.064238\n\n\nLBXHGB\n-0.069491\n0.140983\n0.141013\n0.128587\n0.128571\n-0.002784\n0.058537\n0.057477\n-0.080062\n0.088382\n...\n1.000000\n0.970558\n0.310648\n0.333319\n0.230073\n-0.386739\n-0.134638\n0.021117\n-0.161950\n0.087465\n\n\nLBXHCT\n-0.064322\n0.140536\n0.140560\n0.142448\n0.142436\n-0.025435\n0.079475\n0.066747\n-0.103061\n0.092478\n...\n0.970558\n1.000000\n0.246956\n0.196489\n-0.007125\n-0.308217\n-0.117828\n0.041630\n-0.150902\n0.047683\n\n\nLBXMCVSI\n-0.014322\n0.138290\n0.138275\n0.090765\n0.090806\n-0.029680\n-0.020637\n0.065395\n0.003211\n0.007838\n...\n0.310648\n0.246956\n1.000000\n0.942041\n0.319783\n-0.367893\n-0.194463\n-0.051203\n-0.095972\n0.188139\n\n\nLBXMCHSI\n-0.022460\n0.122018\n0.122016\n0.059423\n0.059451\n0.008186\n-0.046306\n0.044113\n0.033661\n0.002981\n...\n0.333319\n0.196489\n0.942041\n1.000000\n0.616584\n-0.433881\n-0.192630\n-0.070997\n-0.101450\n0.219706\n\n\nLBXMC\n-0.033314\n0.012417\n0.012446\n-0.045515\n-0.045531\n0.093621\n-0.083245\n-0.030895\n0.089158\n-0.008812\n...\n0.230073\n-0.007125\n0.319783\n0.616584\n1.000000\n-0.383169\n-0.090787\n-0.082043\n-0.062251\n0.180371\n\n\nLBXRDW\n0.108660\n0.084576\n0.084538\n0.115427\n0.115424\n0.028080\n-0.053692\n0.026186\n0.036129\n0.000926\n...\n-0.386739\n-0.308217\n-0.367893\n-0.433881\n-0.383169\n1.000000\n0.094689\n0.033970\n0.158203\n-0.174475\n\n\nLBXPLTSI\n0.069111\n0.005530\n0.005544\n-0.101594\n-0.101661\n0.232801\n-0.017074\n-0.164200\n0.044378\n0.025796\n...\n-0.134638\n-0.117828\n-0.194463\n-0.192630\n-0.090787\n0.094689\n1.000000\n-0.393341\n0.162613\n-0.049396\n\n\nLBXMPSI\n-0.004458\n-0.021277\n-0.021291\n0.016290\n0.016333\n0.032040\n-0.019397\n-0.008881\n0.026891\n-0.035289\n...\n0.021117\n0.041630\n-0.051203\n-0.070997\n-0.082043\n0.033970\n-0.393341\n1.000000\n-0.031162\n-0.080035\n\n\nLBXCRP\n0.088166\n0.012294\n0.012298\n-0.014597\n-0.014628\n0.226939\n-0.161871\n-0.037758\n0.156578\n-0.022729\n...\n-0.161950\n-0.150902\n-0.095972\n-0.101450\n-0.062251\n0.158203\n0.162613\n-0.031162\n1.000000\n-0.047023\n\n\nLBXVID\n-0.044873\n-0.007821\n-0.007837\n-0.025229\n-0.025313\n0.054531\n-0.137667\n0.017121\n0.122663\n-0.005901\n...\n0.087465\n0.047683\n0.188139\n0.219706\n0.180371\n-0.174475\n-0.049396\n-0.080035\n-0.047023\n1.000000\n\n\n\n\n27 rows × 27 columns\n\n\n\n\nWe do have some redundant columns in the data now. LBXBCD - Cadmium (ug/L) and LBDBCDSI Cadmium (nmol/L) both measured the same thing, hence the correlation of 1.00 between the two variables. Other redundancies include: LBXBPB and LBDBPBSI, LBXLYPCT and LBDLYMNO, LBXMOPCT and LBDMONO, LBXNEPCT and LBDNENO, LBXBAPCT and LBDBANO, LBXEOPCT and LBDEONO, LBXHGB and LBXHCT, LBXMCHSI and LBXMCVSI and LBXMC\nAlso, note that the depression score is not highly correlated with any of our predictors. It’s highest correlation is 0.108660 to LBXRDW\n\n# drop redundant variables\nto_drop = [\"LBDBPBSI\", \"LBXLYPCT\", \"LBXMOPCT\", \"LBXNEPCT\", \"LBXBAPCT\", \"LBXEOPCT\", \"LBXHCT\", \"LBXMCVSI\", \"LBXMC\"]\nmaster.drop(to_drop, axis = 1, inplace = True)\n\n\nmaster.to_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"/Data/NHANES/master_depression.csv\")",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Initial Modeling of NHANES Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/Elementary_Modeling.html#model-fitting",
    "href": "Applications/Application1/Elementary_Modeling.html#model-fitting",
    "title": "10  Initial Modeling of NHANES Data",
    "section": "10.4 Model Fitting",
    "text": "10.4 Model Fitting\nFirst I will the data set into a training and testing set. The models will be trained and selected using the training set and the final performance will be evaluated using the training set.\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\ntrainX, testX, trainy, testy = train_test_split(master.dropna().drop(\"DepressionScore\", axis = 1), master.dropna()[\"DepressionScore\"], test_size=0.2)\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nlm = LinearRegression().fit(trainX, trainy)\npreds_lm = lm.predict(trainX)\nmse_scores = cross_val_score(lm, trainX, trainy, scoring='neg_mean_squared_error', cv=5)\nprint(\"Mean CV MSE: {:.4f}\".format(-mse_scores.mean()))\nr2_scores = cross_val_score(lm, trainX, trainy, scoring='r2', cv=5)\nprint(\"Mean CV R-Squared: {:.2f}\".format(r2_scores.mean()))\n\nMean CV MSE: 0.2510\nMean CV R-Squared: 0.02\n\n\nR^2 is 0.02 - linear model preforms very poorly. My two assumptions are that either the linear model poorly measures a non-linear relationship or the predictors contain very little information about the mental health of our participants. Let’s try a tree-based method. If the assumption non-linear relationship is the correct one then maybe the tree will do better.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(max_depth = 2)\nrf.fit(trainX, trainy)\nmse_scores = cross_val_score(rf, trainX, trainy, scoring='neg_mean_squared_error', cv=5)\nprint(\"Random Forest Mean CV MSE: {:.4f}\".format(-mse_scores.mean()))\nr2_scores = cross_val_score(rf, trainX, trainy, scoring='r2', cv=5)\nprint(\"Random Forest Mean CV R-Squared: {:.2f}\".format(r2_scores.mean()))\n\nRandom Forest Mean CV MSE: 0.2522\nRandom Forest Mean CV R-Squared: 0.02\n\n\nR^2 is 0.02 - our coefficient of determination does not improve with a non-linear model. This would lead me to believe that our predictors contain no information about the mental health of our participants.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Initial Modeling of NHANES Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application2/boosting.html",
    "href": "Applications/Application2/boosting.html",
    "title": "11  Boosting on CGC Data",
    "section": "",
    "text": "11.1 Pre-processing\nfrom sklearn.model_selection import train_test_split\n# Drop unknown vital status\ndf2 = df[df[\"vital_status\"] != \"Unknown\"]\nX = df.drop(columns = [\"disease_type\", \"vital_status\"])\nbinary_y = df[\"vital_status\"].apply(lambda x: 1 if x == \"Alive\" else 0)\nmulticlass_y = df[\"disease_type\"]\nX_train, X_val, binary_y_train, binary_val = train_test_split(X, binary_y, train_size=0.8, random_state=4)\nX_train2, X_test, binary_y_train2, binary_y_test = train_test_split(X_train, binary_y_train, train_size=0.7, random_state=4)\n#X_train, X_test, multiclass_y_train, multiclass_y_test = train_test_split(X, multiclass_y, train_size=0.7, random_state=4)",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Boosting on CGC Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application2/boosting.html#binary-ada-boost",
    "href": "Applications/Application2/boosting.html#binary-ada-boost",
    "title": "11  Boosting on CGC Data",
    "section": "11.2 Binary ADA Boost",
    "text": "11.2 Binary ADA Boost\nHere we will be using miRNA counts as predictors and vital status (aka whether the patient has survived the cancer (1) or not (0)) as the target.\n\nfrom sklearn.ensemble import AdaBoostClassifier\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, cohen_kappa_score\n\n\nbinary_classifier = AdaBoostClassifier()\nbinary_classifier.fit(X_train2, binary_y_train2)\n\nAdaBoostClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifierAdaBoostClassifier()\n\n\n\n(binary_classifier.decision_function(X_test) &gt;= 0)[:5]\n\narray([False, False,  True,  True,  True])\n\n\n\nbinary_classifier.predict(X_test)[:5]\n\narray([0, 0, 1, 1, 1], dtype=int64)\n\n\nThe decision function essentially gives us the weighted average of the weak classifiers whereas predict returns the sign of the decision function, which in the end is the class predicton.\n\ndef cm_mapper(actual, predictions):\n    import numpy as np\n    mapping = []\n    for i in range(len(predictions)):\n\n        if predictions[i]:\n            if actual[i]:\n                mapping.append(\"TP\")\n            else:\n                mapping.append('FP')\n        else:\n            if actual[i]:\n                mapping.append('FN')\n            else:\n                mapping.append('TN')\n\n    return np.array(mapping)\n\n\nweights = binary_classifier.decision_function(X_test)\npredictions1 = binary_classifier.predict(X_test)\ncm = cm_mapper(predictions1, binary_y_test)\n\nresults = pd.DataFrame({'Actual':binary_y_test, 'Predicted':predictions1, 'Weight':weights, \"Outcome\":cm})\n\n\ngroups = results.groupby('Outcome')\nfor name, group in groups:\n    plt.hist(abs(group.Weight), density=True)\n    plt.title(\"Distribution of Decision Function Values for \" + name)\n    plt.xlabel(\"Decision Function Magnitude\")\n    plt.ylabel(\"Density\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf the magnitude of the decision function is high then you would expect that the model is very sure about the decision it is making. I would assume that these values would surely be true positives and true negatives. Looking at the graphs, it turns out that this assumption is not necessarily true\n\nfpr, tpr, threshold = roc_curve(binary_y_test, predictions1)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(\"Confusion Matrix:\\n\", confusion_matrix(binary_y_test, predictions1)) \nprint(\"Accuracy = \", binary_classifier.score(X_test, binary_y_test))\nprint(\"Kappa = \", cohen_kappa_score(binary_y_test, predictions1))\n\nConfusion Matrix:\n [[34 27]\n [32 75]]\nAccuracy =  0.6488095238095238\nKappa =  0.25383920505871727\n\n\nOur confusion matrix shows a poor accuracy but our kappa value is not awful.\nThe first attempt at boosting has not resulted in a very successful model.\n\nHyperparameter Tuning\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import RidgeClassifier, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Number of estimators\n\nestimators = [10,50,100,150,200]\nscore = []\n\nfor n in estimators:\n\n    classifier = AdaBoostClassifier(n_estimators= n)\n    score.append(cross_val_score(classifier, X_train2, binary_y_train2).mean())\n    \n\n\nplt.plot(estimators, score)\nplt.title(\"Relationship Between Accuracy and Number Estimators\")\nplt.ylabel(\"Cross Validation Accuracy\")\nplt.xlabel(\"Number of Estimators in Boosting Algorithm\")\nplt.show()\n\n\n\n\n\n\n\n\nWe increase our accuracy by roughly 0.01% per estimator which means (assuming linear effects) we would have to add 1000 estimators to get our accuracy from 60% to 70%.\n\n# Learning rate\n\nrates = [0.1,0.25,0.5,0.75,1]\nscore = []\n\nfor r in rates:\n\n    classifier = AdaBoostClassifier(learning_rate=r)\n    score.append(cross_val_score(classifier, X_train2, binary_y_train2).mean())\n\nplt.plot(rates, score)\nplt.title(\"Relationship Between Accuracy and Learning Rate\")\nplt.ylabel(\"Cross Validation Accuracy\")\nplt.xlabel(\"Learning Rate of Boosting Algorithm\")\nplt.show()\n\n\n\n\n\n\n\n\nLowering learning rates give us a boost in accuracy. For our final model we will use a learning rate of 0.1\n\ndef plot_roc(real, predicted, lab):\n    from sklearn.metrics import roc_curve, auc\n    fpr, tpr, threshold2 = roc_curve(real, predicted)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label = lab + ' AUC = %0.2f' % roc_auc)\n    \n\n\n# SAMME vs SAMME.N\n\nbinary_classifier2 = AdaBoostClassifier(algorithm=\"SAMME\")\nbinary_classifier2.fit(X_train2, binary_y_train2)\n\npredictions2 = binary_classifier2.predict(X_test)\n\nplot_roc(binary_y_test, predictions1, \"SAMME.R\")\nplot_roc(binary_y_test, predictions2, \"SAMME\")\n\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(\"SAMME Results\")\n\nprint(\"Confusion Matrix:\\n\", confusion_matrix(binary_y_test, predictions2)) \nprint(\"Accuracy = \", binary_classifier2.score(X_test, binary_y_test))\nprint(\"Kappa = \", cohen_kappa_score(binary_y_test, predictions2))\n\nSAMME Results\nConfusion Matrix:\n [[27 34]\n [32 75]]\nAccuracy =  0.6071428571428571\nKappa =  0.14457645425088717\n\n\nResults are similar. SAMME.R outperforms slightly\n\n# Different classifier \nimport warnings\nwarnings.filterwarnings('ignore')\n\nRidge_booster = AdaBoostClassifier(estimator=RidgeClassifier(),algorithm=\"SAMME\")\nRidge_booster.fit(X_train2, binary_y_train2)\npredictions3 = Ridge_booster.predict(X_test)\n\nLogistic_booster = AdaBoostClassifier(estimator=LogisticRegression(max_iter=100))\nLogistic_booster.fit(X_train2, binary_y_train2)\npredictions4 = Logistic_booster.predict(X_test)\n\nplot_roc(binary_y_test, predictions1, \"Decision Tree\")\nplot_roc(binary_y_test, predictions3, \"Ridge\")\nplot_roc(binary_y_test, predictions4, \"Logistic\")\n\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n\n\n\n\n\n\n\nThe decision tree outperforms the linear classifiers. Let’s see if adding more depth to the decisions trees improves our accuracy (at the loss of interpretibility)\n\nbinary_classifier3 = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2))\nbinary_classifier3.fit(X_train2, binary_y_train2)\n\npredictions5 = binary_classifier3.predict(X_test)\n\nplot_roc(binary_y_test, predictions1, \"Stump\")\nplot_roc(binary_y_test, predictions5, \"Depth 2 Tree\")\n\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n\n\n\n\n\n\n\nAdding depth to the decision tree classifier does not improve the prediction accuracy. After all the hyperparameter tuning, our original model remains strong aside from the an reduction in the learning rate.\n\nfinal_model = AdaBoostClassifier(learning_rate=0.1)\nfinal_model.fit(X_train, binary_y_train)\n\nfinal_predictions = final_model.predict(X_val)\n\nprint(\"Final Model Results\")\nprint(\"Confusion Matrix:\\n\", confusion_matrix(binary_val, final_predictions)) \nprint(\"Accuracy = \", binary_classifier.score(X_val, binary_val))\nprint(\"Kappa = \", cohen_kappa_score(binary_val, final_predictions))\n\nFinal Model Results\nConfusion Matrix:\n [[ 7 47]\n [ 9 77]]\nAccuracy =  0.6\nKappa =  0.0287413280475719\n\n\n\n# Visualization taken from sklearn\n# https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py\n\n\ntwoclass_output = final_model.decision_function(X)\nplot_range = (twoclass_output.min(), twoclass_output.max())\nplt.subplot(122)\nclass_names = [\"Dead\", \"Alive\"]\nplot_colors = \"br\"\nfor i, n, c in zip(range(2), class_names, plot_colors):\n    plt.hist(\n        twoclass_output[binary_y == i],\n        bins=10,\n        range=plot_range,\n        facecolor=c,\n        label=n,\n        alpha=0.5,\n        edgecolor=\"k\",\n    )\nx1, x2, y1, y2 = plt.axis()\nplt.axis((x1, x2, y1, y2 * 1.2))\nplt.legend(loc=\"upper right\")\nplt.ylabel(\"Samples\")\nplt.xlabel(\"Score\")\nplt.title(\"Decision Scores\")\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.35)\nplt.show()",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Boosting on CGC Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application2/boosting.html#variable-importance",
    "href": "Applications/Application2/boosting.html#variable-importance",
    "title": "11  Boosting on CGC Data",
    "section": "11.3 Variable Importance",
    "text": "11.3 Variable Importance\n\ntop10_important = pd.DataFrame({\"miRNA_id\":final_model.feature_names_in_, \"Importance\":final_model.feature_importances_}).sort_values(\"Importance\", ascending=False).head(10)\nplt.barh(top10_important[\"miRNA_id\"], top10_important[\"Importance\"])\nplt.title(\"Top 10 miRNA by importance\")\nplt.xlabel(\"miRNA ID\")\nplt.xlabel(\"Feature Importance\")\nplt.show()",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Boosting on CGC Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application3/boosting2.html",
    "href": "Applications/Application3/boosting2.html",
    "title": "12  Multi-class Boosting on CGC Data",
    "section": "",
    "text": "12.1 Data Pre-processing\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\ndf[\"disease_type\"].value_counts()\n\nMyeloid Leukemias     594\nLymphoid Leukemias    223\nLeukemias, NOS         34\nOther Leukemias         1\nName: disease_type, dtype: int64\nIt seems that there is one observations that belongs to “Other Leukemias” which needs to be dropped. Almost all observations belong to the “Myeloid Leukemias” and “Lymphoid Leukemias” classes.\ndf2 = df[df[\"disease_type\"] != \"Other Leukemias\"]\ndef encode_multiclass(x):\n    \"\"\"\n    Encode the input leukemia type into a numerical value.\n\n    Parameters:\n    - x (str): The input leukemia type.\n\n    Returns:\n    - int: The encoded numerical value.\n      0: If x is \"Myeloid Leukemias\".\n      1: If x is \"Lymphoid Leukemias\".\n      2: For any other input.\n    \"\"\"\n    if x == \"Myeloid Leukemias\":\n        return 0\n    elif x == \"Lymphoid Leukemias\":\n        return 1\n    else:\n        return 2\n# create X and y \nX = df2.drop(columns = [\"disease_type\", \"vital_status\"])\ny = df2[\"disease_type\"].apply(encode_multiclass)\n# split into train, test, and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=4)\nX_train2, X_test, y_train2, y_test = train_test_split(X_train, y_train, train_size=0.7, random_state=4)",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multi-class Boosting on CGC Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application3/boosting2.html#ada-boosting",
    "href": "Applications/Application3/boosting2.html#ada-boosting",
    "title": "12  Multi-class Boosting on CGC Data",
    "section": "12.2 ADA Boosting",
    "text": "12.2 ADA Boosting\nInspiration taken from: https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Define a weak learner using a decision tree with a maximum of 8 leaf nodes\nweak_learner = DecisionTreeClassifier(max_leaf_nodes=8)\n\n# Specify the number of weak learners (trees) in the ensemble\nn_estimators = 300\n\n# Create an AdaBoost classifier using the specified weak learner and parameters\n# - estimator: The base model or weak learner (in this case, a decision tree).\n# - n_estimators: The number of weak learners in the ensemble.\n# - algorithm: The boosting algorithm. \"SAMME\" stands for Stagewise Additive Modeling using a Multiclass Exponential loss function.\n# - random_state: Seed for reproducibility.\n# Fit the AdaBoost classifier to the training data (X_train2, y_train2).\nmodel1 = AdaBoostClassifier(\n    estimator=weak_learner,\n    n_estimators=n_estimators,\n    algorithm=\"SAMME\",\n    random_state=42,\n).fit(X_train2, y_train2)\n\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Create a DummyClassifier as a baseline model\ndummy_clf = DummyClassifier()\n\n# Define a function to calculate misclassification error\ndef misclassification_error(y_true, y_pred):\n    return 1 - accuracy_score(y_true, y_pred)\n\n# Calculate misclassification error for DecisionTreeClassifier (weak learner)\nweak_learners_misclassification_error = misclassification_error(\n    y_test, weak_learner.fit(X_train2, y_train2).predict(X_test)\n)\n\n# Calculate misclassification error for DummyClassifier\ndummy_classifiers_misclassification_error = misclassification_error(\n    y_test, dummy_clf.fit(X_train2, y_train2).predict(X_test)\n)\n\n# Calculate misclassification error for the AdaBoost model (model1)\nboosting_model_misclassification_error = misclassification_error(\n    y_test, model1.predict(X_test)\n)\n\n# Display the results\nprint(\n    \"DecisionTreeClassifier's misclassification error: \"\n    f\"{weak_learners_misclassification_error:.3f}\"\n)\nprint(\n    \"DummyClassifier's misclassification error: \"\n    f\"{dummy_classifiers_misclassification_error:.3f}\"\n)\nprint(\n    \"Boosting Model's misclassification error: \"\n    f\"{boosting_model_misclassification_error:.3f}\"\n)\n\nDecisionTreeClassifier's misclassification error: 0.045\nDummyClassifier's misclassification error: 0.263\nBoosting Model's misclassification error: 0.011\n\n\n\nimport numpy as np\npred = model1.predict(X_test)\nval = y_test\n\n# reverse the mapping of our target variable back into the names of the Leukemia types\nreverse_mapping = {0:\"Myeloid Leukemias\", 1:\"Lymphoid Leukemias\", 2:\"Leukemias, NOS\"}\n\nval = np.array([reverse_mapping[i] for i in val])\n\npred = np.array([reverse_mapping[i] for i in pred])\n\n# Create a confusion matrix of our results\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport matplotlib.pyplot as plt\n\ncm1 = ConfusionMatrixDisplay.from_predictions(val, pred)\n\n\n\n\n\n\n\n\nThis model preforms very well as it only makes 2 wrong predictions out of 180 samples",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multi-class Boosting on CGC Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application3/boosting2.html#variable-importance",
    "href": "Applications/Application3/boosting2.html#variable-importance",
    "title": "12  Multi-class Boosting on CGC Data",
    "section": "12.3 Variable Importance",
    "text": "12.3 Variable Importance\n\nimport pandas as pd\ntop10_important = pd.DataFrame({\"miRNA_id\":model1.feature_names_in_, \"Importance\":model1.feature_importances_}).sort_values(\"Importance\", ascending=False).head(10)\nplt.barh(top10_important[\"miRNA_id\"], top10_important[\"Importance\"], color = \"#152039\")\nplt.title(\"Top 10 miRNA by importance\")\nplt.xlabel(\"miRNA ID\")\nplt.xlabel(\"Feature Importance\")\nplt.show()",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multi-class Boosting on CGC Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application3/boosting2.html#create-model-using-only-top-10",
    "href": "Applications/Application3/boosting2.html#create-model-using-only-top-10",
    "title": "12  Multi-class Boosting on CGC Data",
    "section": "12.4 Create Model Using Only Top 10",
    "text": "12.4 Create Model Using Only Top 10\n\n# remove columns from X that are not in the top most important\nX_test_reduced = X_test.loc[:, top10_important[\"miRNA_id\"]]\nX_train2_reduced = X_train2.loc[:, top10_important[\"miRNA_id\"]]\n\n\n\n# Recreate an AdaBoost classifier using the specified weak learner and parameters\n\n# Fit the AdaBoost classifier to the training data (X_train2_reduced, y_train2).\nmodel2 = AdaBoostClassifier(\n    estimator=weak_learner,\n    n_estimators=n_estimators,\n    algorithm=\"SAMME\",\n    random_state=42,\n).fit(X_train2_reduced, y_train2)\n# Calculate the misclassification error for the new model and display results\nm2_misclassification_error = misclassification_error(\n    y_test, model2.predict(X_test_reduced)\n)\n\nprint(\n    \"Reduced Boosting Model's misclassification_error: \"\n    f\"{m2_misclassification_error:.3f}\"\n)\nprint(\n    \"Boosting Model's misclassification error: \"\n    f\"{boosting_model_misclassification_error:.3f}\"\n)\n\nReduced Boosting Model's misclassification_error: 0.022\nBoosting Model's misclassification error: 0.011\n\n\nOur misclassification error increases by 100% as we reduced the number of input variables in our model. In some cases this is problematic bu our case we are going from 2 misclassified observations to 4 misclassified observations. It is also important to note that we are using 10 input variables instead of 1881\n\n# Reverse the mapping and create a confusion matrix\npred = model2.predict(X_test_reduced)\nval = y_test\n\nreverse_mapping = {0:\"Myeloid Leukemias\", 1:\"Lymphoid Leukemias\", 2:\"Leukemias, NOS\"}\n\nval = np.array([reverse_mapping[i] for i in val])\n\npred = np.array([reverse_mapping[i] for i in pred])\n\ncm2 = ConfusionMatrixDisplay.from_predictions(val, pred)",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multi-class Boosting on CGC Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application3/boosting2.html#illustrate-majority-vote",
    "href": "Applications/Application3/boosting2.html#illustrate-majority-vote",
    "title": "12  Multi-class Boosting on CGC Data",
    "section": "13.1 Illustrate Majority Vote",
    "text": "13.1 Illustrate Majority Vote\n\ninput_prediction = X_val_reduced.iloc[50]\n\nvote = {}\nfor i in range(3):\n    vote[i] = 0\nfor i in final_model.staged_predict(np.array([input_prediction])):\n    vote[i[0]] += 1\n\nvote\n\n{0: 0, 1: 300, 2: 0}\n\n\nWhat I have done is created a dictionairy where the keys are represented by the classes of the target variable and the values are the number of the weak classiifiers which voted for the class\n\ndef create_vote(model, X):\n    \"\"\"\n    Create a dictionary representing the votes from different weak learners in an ensemble model.\n\n    Parameters:\n    - model: The ensemble model with staged predictions.\n    - X: Input data point for which votes are calculated.\n\n    Returns:\n    - dict: A dictionary where keys are class indices and values are the normalized votes for each class.\n    \"\"\"\n    import numpy as np\n\n    # Initialize a dictionary to store votes for each class\n    vote = {}\n\n    # Initialize vote counts for each class to 0\n    for i in range(model.n_classes_):\n        vote[i] = 0\n\n    # Iterate over the staged predictions of the model for the input data point\n    for i in model.staged_predict(np.array([X])):\n        # Increment the vote count for the predicted class\n        vote[i[0]] += 1\n\n    # Normalize the votes by dividing by the total number of weak learners\n    for i in range(model.n_classes_):\n        vote[i] = vote[i] / len(model1.estimator_errors_)\n\n    return vote\n\n\nvote = create_vote(final_model, input_prediction)\n\n\ndef cross_entropy(vote):\n    \"\"\"\n    Calculate the cross-entropy from a dictionary of class probabilities.\n\n    Parameters:\n    - vote: A dictionary where keys are class indices, and values are class probabilities.\n\n    Returns:\n    - float: The calculated cross-entropy.\n    \"\"\"\n    from math import log\n\n    # Initialize the cross-entropy value\n    ce = 0\n\n    # Iterate over the keys (class indices) in the dictionary\n    for i in vote.keys():\n        # Skip classes with zero probability to avoid log(0) issues\n        if vote[i] == 0:\n            continue\n\n        # Update the cross-entropy using the formula: -p * log(p)\n        ce += vote[i] * log(vote[i])\n\n    # Return the negation of the total cross-entropy\n    return -ce\n\nconfidence = 1 - cross_entropy(vote)\nconfidence\n\n1.0\n\n\nI propose cross entropy as a measure of confidence in the predictions of our boosting model. Cross entropy is low when values in a group all belong to the same class. So if all the classifiers in our model vote for the same class then the cross entropy in our vote will be low. If the classifiers do not agree then the cross entropy will rise, maxing out at 0.5. To measure confidence I have subracted the cross entropy from 1 such that values closer to one represent more confidence and confidence falls as the confidence values approaches 0.5.\n\nPlot vote using Pie Chart\n\ntrue = df2.loc[\ny_val.index, \"disease_type\"]\ninput_prediction = X_val_reduced.loc[pred != true].iloc[5]\nvote = create_vote(final_model, input_prediction)\nvote\n\n{0: 0.9466666666666667, 1: 0.05333333333333334, 2: 0.0}\n\n\n\nimport matplotlib.pyplot as plt\n\nlabels = [x[0] for x in vote.items()]\nlabels = [reverse_mapping[i] for i in labels]\nsizes =[x[1] for x in vote.items()]\n\nfig, ax = plt.subplots()\nax.pie(sizes, labels=labels,autopct='%.0f%%',\ncolors=['#5CE1E6', '#FF3131', '#152039'])\nplt.title(\"Boosting Vote for Observation. Confidence: {:.2f}\".format(1-cross_entropy(vote)))\n\nText(0.5, 1.0, 'Boosting Vote for Observation. Confidence: 0.79')",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multi-class Boosting on CGC Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application3/tree_based_zero_inflated_models.html",
    "href": "Applications/Application3/tree_based_zero_inflated_models.html",
    "title": "13  Zero-Inflated Model Using Tree Based Methods",
    "section": "",
    "text": "13.1 Classification of Zeros",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Zero-Inflated Model Using Tree Based Methods</span>"
    ]
  },
  {
    "objectID": "Applications/Application3/tree_based_zero_inflated_models.html#classification-of-zeros",
    "href": "Applications/Application3/tree_based_zero_inflated_models.html#classification-of-zeros",
    "title": "13  Zero-Inflated Model Using Tree Based Methods",
    "section": "",
    "text": "Classification Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassification_model1 = DecisionTreeClassifier()\nclassification_model1.fit(X_train2, y_train2_bin)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\npred = classification_model1.predict(X_test)\nval = y_test_bin\n\n# Create a confusion matrix of our results\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nfrom sklearn.metrics import roc_auc_score\n\nprint(\"Testing ROC AUC\", roc_auc_score(val, pred))\ncm1 = ConfusionMatrixDisplay.from_predictions(val, pred)\ncm1.ax_.set_title(\"Confusion Matrix for Testing Set\")\nplt.show()\n\nTesting ROC AUC 0.4692581435089759\n\n\n\n\n\n\n\n\n\nOur decision tree classifier performs very poorly, in fact it would have been more accurate to predict everything as false. Two explainations of this poor performance are that the classifier is overfitting to the training data or the model is not complex enough to determine the underlying structure of the data. The latter can be solved using more advanced methods such as bagging or random forests. First, the focus will be to determine if our model is overfitting on the training set. Let’s create a confusion matrix for our training data\n\npred = classification_model1.predict(X_train2)\nval = y_train2_bin\n\nprint(\"Training ROC AUC\", roc_auc_score(val, pred))\ncm1 = ConfusionMatrixDisplay.from_predictions(val, pred)\ncm1.ax_.set_title(\"Confusion Matrix for Training Set\")\nplt.show()\n\nTraining ROC AUC 1.0\n\n\n\n\n\n\n\n\n\nObserve from the confusion matrix from the training set that the model is indeed overfitting. There are several hyperparameters that can be tuned to reduce the overfitting such as the maximum depth of the tree, the minimum number of samples required to split a node, or the minimum number of samples required to be a leaf node.\n\nHyperparameter Tuning\n\n# tune tree depth\nfrom sklearn.model_selection import cross_val_score\n\ndepths = range(1,20)\nscore = []\nfor i in depths:\n    clf = DecisionTreeClassifier(max_depth=i)\n    scores = cross_val_score(clf, X_train, y_train_bin, cv=5, scoring = \"roc_auc\")\n    score.append(np.mean(scores))\n\n\nplt.plot(depths, score)\nplt.title(\"Tuning Max Depth of Decision Tree Classifier\")\nplt.xlabel(\"Max Depth\")\nplt.ylabel(\"ROC AUC\")\nplt.show()\n\n\n\n\n\n\n\n\nIt appears that there is no strong trend between the depth of the tree and its roc auc. Let’s try to observe this trend in the training and testing data\n\ntrain_score = []\ntest_score = []\nfor i in depths:\n    clf = DecisionTreeClassifier(max_depth=i)\n    clf.fit(X_train2, y_train2_bin)\n    train_score.append(roc_auc_score(y_train2_bin, clf.predict(X_train2)))\n    test_score.append(roc_auc_score(y_test_bin, clf.predict(X_test)))\n\n\nplt.plot(depths, train_score, label = \"Train\")\nplt.plot(depths, test_score, label = \"Test\")\nplt.legend()\nplt.title(\"Relationship Between Max Depth and ROC AUC\")\nplt.xlabel(\"Max Depth\")\nplt.ylabel(\"ROC AUC\")\nplt.show()\n\n\n\n\n\n\n\n\nThe ROC AUC from both visualization appears close to 0.5 which shows how poorly our model is performing even after we are preventing overfitting. Let’s try to tune the minimum number of samples and see if we get a better result\n\n# tune node split criteria\nvalues = range(1,500,10)\nscore = []\nfor i in values:\n    clf = DecisionTreeClassifier(min_samples_split=i)\n    scores = cross_val_score(clf, X_train, y_train_bin, cv=5, scoring = \"roc_auc\")\n    score.append(np.mean(scores))\n\nplt.plot(values, score)\nplt.title(\"Tuning Min Samples for Split of Decision Tree Classifier\")\nplt.xlabel(\"Min Samples\")\nplt.ylabel(\"ROC AUC\")\nplt.show()\n\n\n\n\n\n\n\n\nThis time we see a weird pattern. The accuracy appears to increase as the complexity of the decision tree decreases. Let’s see if this occurs also in the train data\n\ntrain_score = []\ntest_score = []\nfor i in values:\n    clf = DecisionTreeClassifier(min_samples_split=i)\n    clf.fit(X_train2, y_train2_bin)\n    train_score.append(roc_auc_score(y_train2_bin, clf.predict(X_train2)))\n    test_score.append(roc_auc_score(y_test_bin, clf.predict(X_test)))\n\nplt.plot(values, train_score, label = \"Train\")\nplt.plot(values, test_score, label = \"Test\")\nplt.legend()\nplt.title(\"Relationship Between Min Samples for Split and ROC AUC\")\nplt.xlabel(\"Min Samples\")\nplt.ylabel(\"ROC AUC\")\nplt.show()\n\n\n\n\n\n\n\n\nWe see the same results when comparing the train and test ROC AUCs. We are unable to reduce overfitting by restricting the minimum number of samples required for a split. But from our cross validation we see very interesting reduce. As the number of samples required for a split we actually see the CV ROC AUC increase which is very strange. And even with the increase in AUC, it is not significant enough to the point where we could confidently use the model in practice\nLet’s fit a final decision with 500 samples required for a split and compared it to a random forest classifier\n\nfinal_dtc = DecisionTreeClassifier(min_samples_split=500)\nfinal_dtc.fit(X_train,y_train_bin)\n\npred = final_dtc.predict(X_train)\nval = y_train_bin\n\nprint(\"Training ROC AUC\", roc_auc_score(val, pred))\ncm1 = ConfusionMatrixDisplay.from_predictions(val, pred)\ncm1.ax_.set_title(\"Confusion Matrix for Training Set\")\nplt.show()\n\npred = final_dtc.predict(X_val)\nval = y_val_bin\n\nprint(\"Validation ROC AUC\", roc_auc_score(val, pred))\ncm1 = ConfusionMatrixDisplay.from_predictions(val, pred)\ncm1.ax_.set_title(\"Confusion Matrix for Validation Set\")\nplt.show()\n\nTraining ROC AUC 0.5158881009633793\nValidation ROC AUC 0.4984447900466563\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterestingly, the high ROC AUC from our cross validation does not translate the training and validation. Also note from the confusion matrix, we would be more accurate if we classified all patients as non-depressive. Further, the predictions we make about depressed patients are all incorrect.\n\n\n\nRandom Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassification_model2 = RandomForestClassifier()\nclassification_model2.fit(X_train2, y_train2_bin)\n\nprint(\"Training ROC AUC\", roc_auc_score(y_train2_bin, classification_model2.predict(X_train2)))\ncm1 = ConfusionMatrixDisplay.from_predictions(y_train2_bin, classification_model2.predict(X_train2))\ncm1.ax_.set_title(\"Confusion Matrix for Training Set\")\nplt.show()\n\nprint(\"Testing ROC AUC\", roc_auc_score(y_test_bin, classification_model2.predict(X_test)))\ncm1 = ConfusionMatrixDisplay.from_predictions(y_test_bin, classification_model2.predict(X_test))\ncm1.ax_.set_title(\"Confusion Matrix for Testing Set\")\nplt.show()\n\nTraining ROC AUC 0.9967741935483871\nTesting ROC AUC 0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe random forest classifier results in an increase in test accuracy but only because this model classifies all observations as false. In fact there is an drop in the AUC.\n\nHyperparameter Tuning\n\n# tune tree depth\ndepths = range(1,20)\nscore = []\nfor i in depths:\n    clf = RandomForestClassifier(max_depth=i)\n    scores = cross_val_score(clf, X_train, y_train_bin, cv=5,scoring = \"roc_auc\")\n    score.append(np.mean(scores))\n\nplt.plot(depths, score)\nplt.title(\"Tuning Max Depth of Random Forest Classifier\")\nplt.xlabel(\"Max Depth\")\nplt.ylabel(\"ROC AUC\")\nplt.show()\n\n# tune node split criteria\nvalues = range(1,500,10)\nscore = []\nfor i in values:\n    clf = RandomForestClassifier(min_samples_split=i)\n    scores = cross_val_score(clf, X_train, y_train_bin, cv=5, scoring = \"roc_auc\")\n    score.append(np.mean(scores))\n\nplt.plot(values, score)\nplt.title(\"Tuning Min Samples for Split of Decision Tree Classifier\")\nplt.xlabel(\"Min Samples\")\nplt.ylabel(\"ROC AUC\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see a spike in the ROC AUC for max depth of roughly 7 and the min samples for a split of 400.\n\nfinal_rfc = RandomForestClassifier(max_depth=7, min_samples_split=400)\nfinal_rfc.fit(X_train,y_train_bin)\n\npred = final_rfc.predict(X_train)\nval = y_train_bin\n\nprint(\"Training ROC AUC\", roc_auc_score(val, pred))\ncm1 = ConfusionMatrixDisplay.from_predictions(val, pred)\ncm1.ax_.set_title(\"Confusion Matrix for Training Set\")\nplt.show()\n\npred = final_rfc.predict(X_val)\nval = y_val_bin\n\nprint(\"Validation ROC AUC\", roc_auc_score(val, pred))\ncm1 = ConfusionMatrixDisplay.from_predictions(val, pred)\ncm1.ax_.set_title(\"Confusion Matrix for Validation Set\")\nplt.show()\n\nTraining ROC AUC 0.5\nValidation ROC AUC 0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe random forest classifier predicts all patients as non depressive for a poor 0.5 ROC AUC.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Zero-Inflated Model Using Tree Based Methods</span>"
    ]
  },
  {
    "objectID": "Applications/Application3/tree_based_zero_inflated_models.html#conclusion",
    "href": "Applications/Application3/tree_based_zero_inflated_models.html#conclusion",
    "title": "13  Zero-Inflated Model Using Tree Based Methods",
    "section": "13.2 Conclusion",
    "text": "13.2 Conclusion\nUltimately, none of the models built are viable for the clinical setting they unable to make any accurate predictions on patient’s depression.\nAlso, it is no possible to go further and complete our zero-inflated model when the classification model classifies each observation as 0.\nFor the next application it may be beneficial to mine more input variables from the collection of datasets in NHANES",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Zero-Inflated Model Using Tree Based Methods</span>"
    ]
  },
  {
    "objectID": "Applications/Application4/interpretability.html",
    "href": "Applications/Application4/interpretability.html",
    "title": "14  Interpretability",
    "section": "",
    "text": "14.1 Interpretable Boosting (Cancer Genomics Data)\nIn cancer genome analysis, interpretability in machine learning is vital for ensuring the reliability and acceptance of predictive models. As these models are employed to decipher complex genomic data, understanding the rationale behind predictions is crucial for clinicians, researchers, and patients. Interpretability allows for the identification of biologically relevant features and pathways implicated in cancer progression, aiding in the discovery of potential therapeutic targets. Furthermore, it enables the validation of model predictions against existing medical knowledge, enhancing the clinical utility of these tools. Transparent and interpretable models not only facilitate the integration of AI insights into clinical decision-making but also foster collaboration between computational biologists and medical professionals, fostering a more effective synergy between cutting-edge technology and traditional medical expertise in the pursuit of personalized and precise cancer treatments.\nIn application 2 and 3, I built a boosting model that uses miRNA counts as predictors to classify patients by i) Survival (Y/N) and ii) Tumor Type. In this application, I will use SHAP, LIME, and Interpret ML’s Explainable Boosting Machine API to interpret the results of the boosting models from the previous applications",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "Applications/Application4/interpretability.html#interpretable-boosting-cancer-genomics-data",
    "href": "Applications/Application4/interpretability.html#interpretable-boosting-cancer-genomics-data",
    "title": "14  Interpretability",
    "section": "",
    "text": "Interpret ML’s Explainable Boosting Machine\n\n# Imports \nfrom interpret.glassbox import ExplainableBoostingClassifier\nfrom interpret import show\n\n\n# Load CGC Data\n\ndef load_cgc_metadata():\n    import pandas as pd, os\n    metadata = pd.DataFrame()\n\n    for file in os.listdir(os.path.dirname(os.path.dirname(os.getcwd()))+\"\\\\Data\\\\GeneExpression\\\\\"):\n        if \"manifest\" in file:\n            if metadata.empty:\n                metadata = pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"\\\\Data\\\\GeneExpression\\\\\" + file).loc[:, [\"name\", \"disease_type\", \"id\", 'vital_status']]\n            else:\n                data = pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"\\\\Data\\\\GeneExpression\\\\\" + file).loc[:, [\"name\", \"disease_type\", \"id\", 'vital_status']]\n                metadata = pd.concat([metadata, data])\n\n    metadata = metadata.reset_index(drop= True).drop_duplicates()\n\n    return metadata\n\ndef load_cgc_data():\n    import pandas as pd, os\n\n    metadata = load_cgc_metadata()\n    \n    directory = os.path.dirname(os.path.dirname(os.getcwd())) + \"\\\\Data\\\\GeneExpression\\\\Files\\\\\"\n    master = pd.DataFrame()\n\n    for file in os.listdir(directory):\n        tsv_file_path = directory + file\n\n        df = pd.read_csv(tsv_file_path, sep='\\t')\n        df = pd.DataFrame(df.set_index(\"miRNA_ID\")[\"reads_per_million_miRNA_mapped\"]).rename(columns={\"reads_per_million_miRNA_mapped\":file}).T\n\n        master = pd.concat([master,df])\n    \n    df = metadata.set_index(\"name\").join(master).set_index(\"id\")\n        \n    return df\n\ndf = load_cgc_data()\n\n\n# Pre Processing\ndf = df[df[\"disease_type\"] != \"Other Leukemias\"] # Remove Other Leukemias from target variable\nX = df.drop(columns = [\"disease_type\", \"vital_status\"]) # Create X\nbinary_y = df[\"vital_status\"] # Create the y vector for binary classification of Survival Status\nmulticlass_y = df[\"disease_type\"] # Create the y vector for multi class classification of tumor type\n\n\n# Train Explainable Boosting Machines\n\nebm1 = ExplainableBoostingClassifier()\nebm1.fit(X, binary_y==\"Alive\")\n\nebm2 = ExplainableBoostingClassifier()\nebm2.fit(X, multiclass_y)\n\n# This was running for 20 minutes+ so I am aborting this section of analysis for now. If there is time at the end I will try to get to it\n\n\n# Local Interpretation for Multi Class Boosting \n\nebm_local1 = ebm1.explain_local(X, binary_y)\nshow(ebm_local1)\n\nNameError: name 'ebm1' is not defined\n\n\n\n# Local Interpretation for Multi Class Boosting \n\nebm_local1 = ebm1.explain_local(X, mutliclass_y)\nshow(ebm_local1)\n\n\n\nInterpreting Boosting Using LIME\nLIME, or Local Interpretable Model-agnostic Explanations, is a technique in machine learning designed to provide interpretability for complex models by approximating their predictions through simpler, locally faithful models. It works by perturbing input data and observing the corresponding changes in model outputs, generating interpretable explanations that help users understand the decision-making process of black-box models on a local scale.\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom interpret.blackbox import LimeTabular\n\n\n# boosting2 = AdaBoostClassifier(estimator = DecisionTreeClassifier(max_leaf_nodes=8),\n#                                n_estimators=300,\n#                                algorithm=\"SAMME\",\n#                                random_state=42).fit(X,multiclass_y)\n\n# lime = LimeTabular(boosting2, X)\n\n# show(lime.explain_local(X[:5], multiclass_y[:5]),0)\n\nThe above code produces an error that multiclass is not supported by LIME and the same goes with SHAP. This means I will look only at the interpretability of binary classfication of patients into their Survival Status\n\n# LIME in binary ADABOOST classification\nboosting1 = AdaBoostClassifier(learning_rate = 0.01).fit(X,binary_y == \"Alive\") # train model\n\nlime = LimeTabular(boosting1, X) # set up Lime object\nshow(lime.explain_local(X[:5]),0) # create local explanations\n\n\n\n\n\nThe local explanations produced by the above LIME model is significant for several reasons: - The stakeholders of this model are more likely to trust and adopt it if they can understand how it arrives at its predictions. Trust is important, especially in the health care spaced where the consequences asssociated with errors are significant - There are many regulations within the health care space that require transparency and explainability in decision-making processes. - From a legal prespective, having an interpretable model is essential. It helps in providing evidence and explainations for the decisions made, which can be crucial for legal defense and protecting an organization’s reputation. This is especially significant in the health care industry, where stocks of companies will spike or plument based on legal proceedings regarding to their products.\n\n# Analysis of False Classifications\n\nincorrect = boosting1.predict(X) != (binary_y == \"Alive\") # filter incorrect classifications\n \nshow(lime.explain_local(X[incorrect][:10], binary_y[incorrect][:10]== \"Alive\"), 0)\n\n\n\n\n\nI have filtered all the incorrect predictions and created LIME intepretations for these observations. The results from these interpretations is significant because: - they allow data scientists to identify issues and improve model performance. Decision-making processes can be traced allowing problems to be diagnosed more effectively - it provides insight into feature importance. Understanding which features contribute to a model’s predictions and errors is valuable information. This insight can enhance the overall understanding of whatever the domain in which we are working in.\n\n\nInterpreting Boosting Using SHAP\nShap is not compatible with ADABoost so we will only implement it on our random forest classifier for the NHANEs",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  },
  {
    "objectID": "Applications/Application4/interpretability.html#interpretable-random-forest-nhanes",
    "href": "Applications/Application4/interpretability.html#interpretable-random-forest-nhanes",
    "title": "14  Interpretability",
    "section": "14.2 Interpretable Random Forest (NHANES)",
    "text": "14.2 Interpretable Random Forest (NHANES)\nJust as in cancer genome analysis, interpretabile machine learning in the diagnosis of depression and other medical illnesses is vital for ensuring the reliability and acceptance of predictive models. Interpretable models provide insights into the features and patterns that contribute to a particular diagnosis, allowing clinicians to validate and comprehend the decision-making process. This transparency is vital for ensuring the reliability and accuracy of predictions, as well as for identifying potential biases or errors that could have significant consequences in a medical context. Additionally, interpretable models enable healthcare professionals to integrate their domain expertise with the machine-generated insights, fostering a collaborative and synergistic approach to medical diagnosis. Ultimately, interpretability enhances the accountability and acceptance of machine learning models in healthcare, contributing to more accurate and transparent diagnostic processes.\nIn the previous applications I attempted to predict depression risk using blood and vitamin levels among other biological health factors. Now, I will look to use SHAP to interpret the predictions made by these models.\n\n# Load data and pre process\nimport os, pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\n\nmaster = pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"/Data/NHANES/master_depression.csv\").set_index(\"SEQN\")\nmaster.dropna(inplace=True)\nmaster.drop(columns=['LBXBCD', 'LBXWBCSI', 'LBXBCD', 'LBXRBCSI', \"LBXWBCSI\", 'LBXBCD', 'LBXRBCSI', 'LBXWBCSI', 'LBXBCD'], inplace=True)\nX2 = master.drop(columns=\"DepressionScore\")\ny = master[\"DepressionScore\"]\nbinary_y2 = y &gt; 1\n\n\n# Build model\nrfc = RandomForestClassifier(max_depth=7).fit(X2, binary_y2)\n\n\nInterpreting Random Forest Using SHAP\nSHAP (SHapley Additive exPlanations) is a popular interpretability framework in machine learning that aims to attribute the contribution of each feature to a model’s prediction. It leverages Shapley values from cooperative game theory to fairly distribute the overall prediction among individual features, providing insights into how each input variable influences the model’s output.\n\n# Global Importance\nimport shap\nexplainer = shap.TreeExplainer(rfc)\nshap_values = explainer.shap_values(X2)\nshap.summary_plot(shap_values, X2, plot_type=\"bar\")\n\n\n\n\n\n\n\n\nThe above plot depicts the global importance of the features as a function of the mean shap value across all observations. This graph allows human operators to understand the model’s decision on a broad scale which can enhance the trust of the user and the overall understanding in the domain. It is also important to note the ability to discriminate the influence between predictions on different classes.\n\n# local importance\ninstance = X2.iloc[0] # define instance\nvalues = explainer.shap_values(instance) # determine shap values\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], values[1], instance) \n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\nThe above plot depicts the local importance of the variables on a single prediction. The pink features cause an increase in the prediction probability and the length of the bar shows the magnitude of the feature’s shap value. The blue features, on the other hand, cause a decrease in the prediciton probability.\nThis plot is useful because: - it allows stakeholders to understand how the model arrives at its predictions which improves trust and adoption - again, it enables the model to comply with regulations enforced in the health care and helps in providing evidence in legal circumstances - it can effectively integrate into decision-making in the domain. This is just another tool therapists (or doctors in general) can use to make accurate diagnoses",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Interpretability</span>"
    ]
  }
]