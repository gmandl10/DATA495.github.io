[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "1 Introduction",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R1.html",
    "href": "Weekly_Reports/R1.html",
    "title": "2  DATA495 Weekly Report 1",
    "section": "",
    "text": "3 Weekly Report 1\nDate: January 19, 2024",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DATA495 Weekly Report 1</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R1.html#past-objective-status",
    "href": "Weekly_Reports/R1.html#past-objective-status",
    "title": "2  DATA495 Weekly Report 1",
    "section": "3.1 Past Objective Status",
    "text": "3.1 Past Objective Status\nN/A",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DATA495 Weekly Report 1</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R1.html#weekly-work-log",
    "href": "Weekly_Reports/R1.html#weekly-work-log",
    "title": "2  DATA495 Weekly Report 1",
    "section": "3.2 Weekly Work Log",
    "text": "3.2 Weekly Work Log\n\n\n\n\n\n\n\n\nActivity\nDescription\nTime Allocated\n\n\n\n\nApplication 1\nSearch for dataset, data preprocessing in Python, exploratory data analysis\n2 hours\n\n\nElements of Statistical Learning\n9.1 Generalized Additive Models, 9.4 Multivariate Adaptive Regression Splines\n2 hours",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DATA495 Weekly Report 1</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R1.html#upcoming-objectives",
    "href": "Weekly_Reports/R1.html#upcoming-objectives",
    "title": "2  DATA495 Weekly Report 1",
    "section": "3.3 Upcoming Objectives",
    "text": "3.3 Upcoming Objectives\n\n\n\n\n\n\n\nObjective\nDescription\n\n\n\n\nLiterature Review\nExplore explainability of GAMs and spline based methods, application of machine learning in precision medicine (specifically GAMs and splines)\n\n\nElements of Statistical Learning\nRead Chapter 10: Boosting and Additive Trees\n\n\nApplication 1\nPrepare elementary statistical models which we can improve on with the next application assignments",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DATA495 Weekly Report 1</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R2.html",
    "href": "Weekly_Reports/R2.html",
    "title": "3  DATA495 Weekly Report 2",
    "section": "",
    "text": "4 Weekly Report 2\nDate: January 26, 2024",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DATA495 Weekly Report 2</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R2.html#past-objective-status",
    "href": "Weekly_Reports/R2.html#past-objective-status",
    "title": "3  DATA495 Weekly Report 2",
    "section": "4.1 Past Objective Status",
    "text": "4.1 Past Objective Status\n\n\n\n\n\n\n\n\nObjective\nDescription\nStatus\n\n\n\n\nLiterature Review\nExplore explainability of GAMs and spline based methods, application of machine learning in precision medicine (specifically GAMs and splines)\nComplete, see below\n\n\nElements of Statistical Learning\nRead Chapter 10: Boosting and Additive Trees\nPartially Complete\n\n\nApplication 1\nPrepare elementary statistical models which we can improve on with the next application assignments\nComplete",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DATA495 Weekly Report 2</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R2.html#weekly-work-log",
    "href": "Weekly_Reports/R2.html#weekly-work-log",
    "title": "3  DATA495 Weekly Report 2",
    "section": "4.2 Weekly Work Log",
    "text": "4.2 Weekly Work Log\n\n\n\nActivity\nDescription\nTime Allocated\n\n\n\n\nApplication 1\nExploratory Data Analysis, Preliminary Modeling\n2 hours\n\n\nElements of Statistical Learning\nChapter 10.1-10.4\n1 hour\n\n\nLiterature Review\nSee below\n1 hour",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DATA495 Weekly Report 2</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R2.html#upcoming-objectives",
    "href": "Weekly_Reports/R2.html#upcoming-objectives",
    "title": "3  DATA495 Weekly Report 2",
    "section": "4.3 Upcoming Objectives",
    "text": "4.3 Upcoming Objectives\n\n\n\n\n\n\n\nObjective\nDescription\n\n\n\n\nLiterature Review\nExplore explain ability boosting, application of machine learning in NHANES and precision medicine (specifically GAMs and boosting)\n\n\nElements of Statistical Learning\nFinish reading Chapter 10: Boosting and Additive Trees\n\n\nApplication 2\nPrepare applications of GAMs to NHANES data",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DATA495 Weekly Report 2</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R2.html#literature-review",
    "href": "Weekly_Reports/R2.html#literature-review",
    "title": "3  DATA495 Weekly Report 2",
    "section": "4.4 Literature Review",
    "text": "4.4 Literature Review\n\n\n\n\n\n\n\n\nTitle\nISSN\nComments\n\n\n\n\nMachine Learning Model for Predicting CVD Risk on NHANES Data (search: machine learning NHANEs)\n2694-0604\nclassify NHANES participants into 3 categories: Healthy, non-healthy, needs further tests.used SVM with radial kernel as classifier.tested independently on 4 different ages groups.Let’s reproduce this with different types of machine learning models and try to analyze interpretability of our classifications\n\n\nGAMI-Net: An explainable neural network based on generalized additive models with structured interactions (search: explainability generalized additive models)\n0031-3203\ncontends to balance trade-off of prediction accuracy (from neural networks) and model interpretability (from GAMs) interpretability aspects: sparsity - only including the most important effects, heredity - interactions only include if parent terms are already present in the model, marginal clarity - “to make main effects and pairwise interactions mutually distinguishable” (not sure what this is supposed to mean) reduces the redundancy of the main terms and their presence in interactions.training of the model occurs in two steps: first model is trained on main effects and then interaction terms are fit to residuals.GAMI-net allows you to get local interpretations for individual observations.*Can calculate the importance ratio (IR) for each effect and use it to determine how important a variable was in 1. the entire model 2. in an individual observation. The higher the IR, the more important",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DATA495 Weekly Report 2</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R3.html",
    "href": "Weekly_Reports/R3.html",
    "title": "4  DATA495 Weekly Report 3",
    "section": "",
    "text": "5 Weekly Report 3\nDate: February 2nd, 2024",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DATA495 Weekly Report 3</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R3.html#past-objective-status",
    "href": "Weekly_Reports/R3.html#past-objective-status",
    "title": "4  DATA495 Weekly Report 3",
    "section": "5.1 Past Objective Status",
    "text": "5.1 Past Objective Status\n\n\n\n\n\n\n\n\nObjective\nDescription\nStatus\n\n\n\n\nLiterature Review\nExplore explain ability boosting, application of machine learning in NHANES and precision medicine (specifically GAMs and boosting)\nComplete, see below\n\n\nElements of Statistical Learning\nRead Chapter 10: Boosting and Additive Trees\nPartially Complete\n\n\nApplication 1\nPrepare elementary statistical models which we can improve on with the next application assignments\nComplete",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DATA495 Weekly Report 3</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R3.html#weekly-work-log",
    "href": "Weekly_Reports/R3.html#weekly-work-log",
    "title": "4  DATA495 Weekly Report 3",
    "section": "5.2 Weekly Work Log",
    "text": "5.2 Weekly Work Log\n\n\n\nActivity\nDescription\nTime Allocated\n\n\n\n\nApplication 2\nNew dataset loading, boosting and mars\n2 hours\n\n\nElements of Statistical Learning\nChapter 10 Boosting\n1 hour\n\n\nLiterature Review\nSee below\n1.5 hour\n\n\nGitHub setup\n/\n1 hour",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DATA495 Weekly Report 3</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R3.html#upcoming-objectives",
    "href": "Weekly_Reports/R3.html#upcoming-objectives",
    "title": "4  DATA495 Weekly Report 3",
    "section": "5.3 Upcoming Objectives",
    "text": "5.3 Upcoming Objectives\n\n\n\n\n\n\n\nObjective\nDescription\n\n\n\n\nLiterature Review\nExplainable/Trustworthy AI in Medicine - what are the requirements and how can we build models that meet these requirements\n\n\nElements of Statistical Learning\nDiscuss topics with Devan in weekly meeting\n\n\nApplication 2\nDue Monday night, complete analysis of boosting and mars models.",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DATA495 Weekly Report 3</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R3.html#literature-review",
    "href": "Weekly_Reports/R3.html#literature-review",
    "title": "4  DATA495 Weekly Report 3",
    "section": "5.4 Literature Review",
    "text": "5.4 Literature Review\n\n\n\n\n\n\n\n\nTitle\nISSN\nComments\n\n\n\n\nMining the breast cancer pattern using artificial neural networks and multivariate adaptive regression splines\n0957-4174\nIn this paper, the authors suggest using MARS for determining important variables and then only using these variables as input in a neural network. This neural network, although more complex, performs worse than the MARS model on out of sample classification of breast cancer patients. Maybe interactions are significant in this data and MARS is able to pick these up better than an ANN.\n\n\nMultivariate adaptive regression splines analysis to predict biomarkers of spontaneous preterm birth\n1600-0412\nFocus of this analysis was not so much on importance of variables but the presence of certain variables in models for Caucasians and for African Americans. They looked at a MARS model for all the data and the basis functions and then the same for both races. Interesting is that the model for African Americans has a significantly better AUC than the model for Caucasians although the sample size for both categories is approximately the same\n\n\nAdaptive splines-based logistic regression with a ReLU neural network\nhttps://hal.science/hal-03778323\nThe authors tried to combine the neural network structure but at each neuron they looked at the interaction between 2 knots. The formula for the model looks very similar to the MARS formula. Inside the activation function of the suggested NN-MARS model we add various components from the collection of basis functions C, whereas in MARS the activation function contains the products of functions in C. The results are nothing special: a regular neural network outpreforms the NN-MARS and quite similarily to a MARS model in both accuracy and AUC. Also the fitting of the NN-MARS model takes longer on average than a NN.",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DATA495 Weekly Report 3</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R4.html",
    "href": "Weekly_Reports/R4.html",
    "title": "5  DATA495 Weekly Report 4",
    "section": "",
    "text": "6 Weekly Report 4\nDate: February 9th, 2024",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DATA495 Weekly Report 4</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R4.html#past-objective-status",
    "href": "Weekly_Reports/R4.html#past-objective-status",
    "title": "5  DATA495 Weekly Report 4",
    "section": "6.1 Past Objective Status",
    "text": "6.1 Past Objective Status\n\n\n\n\n\n\n\n\nObjective\nDescription\nStatus\n\n\n\n\nLiterature Review\nExplainable/Trustworthy AI in Medicine - what are the requirements and how can we build models that meet these requirements\nComplete, see below\n\n\nElements of Statistical Learning\nDiscuss topics with Devan in weekly meeting\n\n\n\nApplication 2\nDue Monday night, complete analysis of boosting and mars models.",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DATA495 Weekly Report 4</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R4.html#weekly-work-log",
    "href": "Weekly_Reports/R4.html#weekly-work-log",
    "title": "5  DATA495 Weekly Report 4",
    "section": "6.2 Weekly Work Log",
    "text": "6.2 Weekly Work Log\n\n\n\n\n\n\n\n\nActivity\nDescription\nTime Allocated\n\n\n\n\nApplication 2\nBoosting and zero inflated models\n3 hours\n\n\nElements of Statistical Learning\nChapter 9.2 Tree-Based Methods\n45 minutes\n\n\nLiterature Review\nSee below\n1 hour",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DATA495 Weekly Report 4</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R4.html#upcoming-objectives",
    "href": "Weekly_Reports/R4.html#upcoming-objectives",
    "title": "5  DATA495 Weekly Report 4",
    "section": "6.3 Upcoming Objectives",
    "text": "6.3 Upcoming Objectives\n\n\n\nObjective\nDescription\n\n\n\n\nLiterature Review\nExplainable tree based methods\n\n\nElements of Statistical Learning\n9.3 Prim Bump Hunting\n\n\nApplication 3\nImplement explainable trees\n\n\nMidterm Knowledge Report\nPrepare infographics",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DATA495 Weekly Report 4</span>"
    ]
  },
  {
    "objectID": "Weekly_Reports/R4.html#literature-review",
    "href": "Weekly_Reports/R4.html#literature-review",
    "title": "5  DATA495 Weekly Report 4",
    "section": "6.4 Literature Review",
    "text": "6.4 Literature Review\n\n\n\n\n\n\n\n\nTitle\nLink\nComments\n\n\n\n\nImproving the explainability of Random Forest classifier – user centered approach\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728671/\nThis article is extremely relevant to this course and my future interests in ML in the health care industry. It outlines the process of creating explainable machine learning tools for use by industry professionals. The authors began their work with discussions with practitioners regarding their expectations for relying on AI in their work. The authors created a one-page graphic that addressing all the expectations and enhances the decisions made by practitioners. I really like this article because it provides a framework for how we can incorporate AI into the bio sciences.\n\n\nPrediction of slaughter age in pigs and assessment of the predictive value of phenotypic and genetic information using random forest\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6276566/\nI found this article because it was cited the previous article and I wanted to find further extensions of explainable interfaces in the context of biosciences, specifically genetics. These authors attempted to predict pig slaughter age using phenotype factors and although successful, did not explicitly create a explainability interface. They included a graph of the feature importances but this does not solve the problem of making machine learning results understandable for non-expert users from the industry. They only referenced the previous article in the discussion as a framework for enhancing explainability.",
    "crumbs": [
      "Weekly Report",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DATA495 Weekly Report 4</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html",
    "href": "Applications/Application1/EDA.html",
    "title": "6  Data Pre-Processing",
    "section": "",
    "text": "6.1 DS0214\nDietary Supplement Use – Supplement Information\nThis data set stores dietary supplement information. The primary key to this dataset is the Supplement ID number. Let’s see if this is a foreign key in another dataset\nfor i in dataframes.index:\n    df = dataframes[i]\n    \n    if \"DSDSUPID\" in df.columns:\n        print(i)\n\nDS0213\nDS0214\nDS0215\nThe Supplement ID number (DSDSUPID) is the primary key in DS0214 and is a foreign key in DS0213\nDS0213 is a dataset that contains the serial number and the data release number. DSO213 is Dietary Supplement Use – Participants Use of Supplement Let’s see how DSO213 is structured: Can one person be recorded taking multiple supplements? If so, what is the distribution of supplement usage in the same?\nDS2013 = pd.read_csv(\"ICPSR_25504\\\\DS0213\\\\25504-0213-Data.tsv\", sep = \"\\t\")\nsupplement_usage = DS2013.groupby(\"SEQN\").size().sort_values(ascending = False).reset_index(name=\"Count\")\nsupplement_usage\n\n\n\n\n\n\n\n\nSEQN\nCount\n\n\n\n\n0\n34656\n20\n\n\n1\n40551\n19\n\n\n2\n37134\n16\n\n\n3\n38637\n16\n\n\n4\n37164\n14\n\n\n...\n...\n...\n\n\n4105\n35518\n1\n\n\n4106\n35522\n1\n\n\n4107\n35524\n1\n\n\n4108\n35525\n1\n\n\n4109\n36307\n1\n\n\n\n\n4110 rows × 2 columns\nimport matplotlib.pyplot as plt\nplt.hist(supplement_usage[\"Count\"], bins = 12)\nplt.xlabel(\"Number of Supplements Used\")\nplt.ylabel(\"Number of Participants\")\nplt.title(\"Distribution of Supplement Usage Among Study Participants\")\nplt.show()\nA key thing to note is that a participant who is not using supplements is not included in this visualization. This analysis is only representative of a participant that uses one or multiple supplement(s)\nLet’s look at the next dataset that does not contain the sequence number or data release number",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#ds0215",
    "href": "Applications/Application1/EDA.html#ds0215",
    "title": "6  Data Pre-Processing",
    "section": "6.2 DS0215",
    "text": "6.2 DS0215\nDietary Supplement Use – Ingredient Information\nThe Supplement ID number (DSDSUPID) is also the primary key in DS0214 and is a foreign key in DS0213\nCan a supplement have several ingredients and what is the distributions of number ingredients in a supplements\n\nDS2015 = pd.read_csv(\"ICPSR_25504\\\\DS0215\\\\25504-0215-Data.tsv\", sep = \"\\t\")\nnum_ingredients = DS2015.groupby(\"DSDSUPP\").size().sort_values(ascending = False).reset_index(name=\"Count\")\nnum_ingredients\n\n\n\n\n\n\n\n\nDSDSUPP\nCount\n\n\n\n\n0\nS1000579801\n68\n\n\n1\nS1000579800\n65\n\n\n2\nS1000580200\n65\n\n\n3\nS1000600500\n65\n\n\n4\nS1000582500\n64\n\n\n...\n...\n...\n\n\n2138\nS1000649000\n1\n\n\n2139\nS1000649100\n1\n\n\n2140\nS1000649200\n1\n\n\n2141\nS1000649300\n1\n\n\n2142\nS1888690300\n1\n\n\n\n\n2143 rows × 2 columns\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.hist(num_ingredients[\"Count\"], bins = 12)\nplt.xlabel(\"Number of Ingredients in Supplement\")\nplt.ylabel(\"Number of Supplements\")\nplt.title(\"Distribution of Amount of Ingredients Among Supplements\")\nplt.show()\n\n\n\n\n\n\n\n\n\nDS2015[DS2015[\"DSDSUPP\"]==\"S1000579801\"]\n\n\n\n\n\n\n\n\nDSDSUPID\nDSDSUPP\nDSDINGID\nDSDINGR\nDSDOPER\nDSDQTY\nDSDUNIT\nDSDCAT\nDSDBLFLG\n\n\n\n\n11661\n1000579801\nS1000579801\n10000037\nI10000037\n=\n100.0\n1\n4\n2\n\n\n11662\n1000579801\nS1000579801\n10000038\nI10000038\n=\n50.0\n1\n4\n2\n\n\n11663\n1000579801\nS1000579801\n10000042\nI10000042\n=\n150.0\n4\n1\n2\n\n\n11664\n1000579801\nS1000579801\n10000052\nI10000052\n=\n1.5\n1\n2\n2\n\n\n11665\n1000579801\nS1000579801\n10000070\nI10000070\n=\n250.0\n1\n2\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11724\n1000579801\nS1000579801\n10004592\nI10004592\n=\n400.0\n1\n4\n2\n\n\n11725\n1000579801\nS1000579801\n10004593\nI10004593\n=\n100.0\n1\n4\n2\n\n\n11726\n1000579801\nS1000579801\n10004595\nI10004595\n=\n100.0\n1\n4\n2\n\n\n11727\n1000579801\nS1000579801\n10004927\nI10004927\n=\n15.0\n1\n3\n2\n\n\n11728\n1000579801\nS1000579801\n10004928\nI10004928\n=\n125.0\n1\n4\n2\n\n\n\n\n68 rows × 9 columns",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#ds0216",
    "href": "Applications/Application1/EDA.html#ds0216",
    "title": "6  Data Pre-Processing",
    "section": "6.3 DS0216",
    "text": "6.3 DS0216\nDietary Supplement Use – Supplement Blend\nThe primary key in DS0216 is the Ingredient ID number which is a foreign key in DS0215",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#ds0235",
    "href": "Applications/Application1/EDA.html#ds0235",
    "title": "6  Data Pre-Processing",
    "section": "6.4 DS0235",
    "text": "6.4 DS0235\nDrug Information\nThe primary key for DS0235 is the Generic Drug Code (RXDDRGID). Let’s find which dataset has the generic drug code as a foreign key.\n\nfor i in dataframes.index:\n    df = dataframes[i]\n    \n    if \"RXDDRGID\" in df.columns:\n        print(i)\n\nDS0234\nDS0235\n\n\nDS0234 contains RXDDRGID as a foreign key. This dataset is Prescription Medications\n\nDS2034 = pd.read_csv(\"ICPSR_25504\\\\DS0234\\\\25504-0234-Data.tsv\", sep = \"\\t\")\nnum_drugs = DS2034.groupby(\"SEQN\").size().sort_values(ascending = False).reset_index(name=\"Count\")\nnum_drugs\n\n\n\n\n\n\n\n\nSEQN\nCount\n\n\n\n\n0\n41025\n20\n\n\n1\n34531\n18\n\n\n2\n34423\n17\n\n\n3\n38612\n16\n\n\n4\n33959\n16\n\n\n...\n...\n...\n\n\n10343\n34985\n1\n\n\n10344\n34984\n1\n\n\n10345\n34983\n1\n\n\n10346\n34982\n1\n\n\n10347\n36301\n1\n\n\n\n\n10348 rows × 2 columns\n\n\n\n\n# Partipicant 41025 uses 20 perscription drugs\nDS2034[DS2034[\"SEQN\"] == 41025]\n\n\n\n\n\n\n\n\nSEQN\nRXDUSE\nRXDDRUG\nRXDDRGID\nRXQSEEN\nRXDDAYS\nRXDCOUNT\nSDDSRVYR\nRIDSTATR\nRIDEXMON\n...\nFIAPROXY\nFIAINTRP\nMIALANG\nMIAPROXY\nMIAINTRP\nAIALANG\nWTINT2YR\nWTMEC2YR\nSDMVPSU\nSDMVSTRA\n\n\n\n\n17125\n41025\n1\nACETAMINOPHEN; HYDROCODONE\nd03428\n1\n1825\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17126\n41025\n1\nAZELASTINE NASAL\nd04068\n1\n182\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17127\n41025\n1\nBUSPIRONE\nd00182\n1\n122\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17128\n41025\n1\nCHOLESTYRAMINE\nd00193\n1\n547\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17129\n41025\n1\nCITALOPRAM\nd04332\n1\n243\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17130\n41025\n1\nDILTIAZEM\nd00045\n1\n365\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17131\n41025\n1\nESOMEPRAZOLE\nd04749\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17132\n41025\n1\nFLUTICASONE\nd01296\n1\n1825\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17133\n41025\n1\nFLUTICASONE; SALMETEROL\nd04611\n1\n1095\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17134\n41025\n1\nGABAPENTIN\nd03182\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17135\n41025\n1\nGUAIFENESIN; PSEUDOEPHEDRINE\nd03379\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17136\n41025\n1\nLORAZEPAM\nd00149\n1\n365\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17137\n41025\n1\nMETAXALONE\nd00964\n1\n1460\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17138\n41025\n1\nNABUMETONE\nd00310\n1\n243\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17139\n41025\n1\nOXYBUTYNIN\nd00328\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17140\n41025\n1\nOXYCODONE\nd00329\n1\n1825\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17141\n41025\n1\nQUININE\nd00366\n1\n10950\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17142\n41025\n1\nSUMATRIPTAN\nd03160\n1\n1825\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17143\n41025\n1\nTAMSULOSIN\nd04121\n1\n730\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n17144\n41025\n1\nTRAZODONE\nd00395\n1\n365\n20\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n98473.732756\n100919.498403\n2\n47\n\n\n\n\n20 rows × 49 columns\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.hist(num_drugs[\"Count\"], bins = 10)\nplt.xlabel(\"Number of Perscription Drugs Used\")\nplt.ylabel(\"Number of Participants\")\nplt.title(\"Perpsection Drug Usage Among Study Participants\")\nplt.show()",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#year-to-year-survey-data-consistency",
    "href": "Applications/Application1/EDA.html#year-to-year-survey-data-consistency",
    "title": "6  Data Pre-Processing",
    "section": "6.5 Year to Year Survey Data Consistency",
    "text": "6.5 Year to Year Survey Data Consistency\nThe first data that we have been working with from the dataset ICPSR_25504 is the National Health and Nutrition Examination Survey (NHANES) from 2005-2006. Next we have downloaded the Survey from 2005-2006 and we will look to see if the data architecture is consistent across year.\nFirst let’s check if each data set exists in both surveys and if their columns are the same\n\nimport os\nimport pandas as pd\n\ndata_master = \"ICPSR_25505\\\\\"\ndataframes2 = []\nindex2 = []\ni = 0\n\nos.listdir(data_master)\nfor root, dirs, files in os.walk(data_master):\n    for name in files:\n        if name.endswith(\".tsv\"):\n            file = os.path.join(root, name)\n            dataframes2.append(pd.read_csv(file, sep = \"\\t\", nrows = 100))\n            index2.append(root[-6:])\n\ndataframes2 = pd.Series(dataframes2, index = index2)\n\n\nprint(\"Datasets included in 2005-2006 but not in 2007-2008\")\nfor i in index:\n    if i not in index2:\n        print(i)\n\n\nprint(\"Datasets not included in 2005-2006 but in 2007-2008\")\nfor i in index2:\n    if i not in index:\n        print(i)\n\nprint(\"Datasets with mismatching columns\")\nfor i in index:\n    if i in index2:\n        if len(dataframes[i].columns) != len(dataframes2[i].columns):\n            print(i)\n        \nprint(\"Datasets with consistent number of columns\")\nfor i in index:\n    if i in index2:\n        if len(dataframes[i].columns) == len(dataframes2[i].columns):\n            print(i)\n        \n\nDatasets included in 2005-2006 but not in 2007-2008\nDS0018\nDS0019\nDS0020\nDS0021\nDS0022\nDS0023\nDS0024\nDS0025\nDS0026\nDS0128\nDS0129\nDS0130\nDS0131\nDS0132\nDS0133\nDS0134\nDS0135\nDS0136\nDS0137\nDS0138\nDS0139\nDS0140\nDS0141\nDS0142\nDS0143\nDS0144\nDS0234\nDS0235\nDS0236\nDS0237\nDS0238\nDS0239\nDS0240\nDS0241\nDS0242\nDS0243\nDS0244\nDS0245\nDS0246\nDS0247\nDS0248\nDatasets not included in 2005-2006 but in 2007-2008\nDS0100\nDatasets with mismatching columns\nDS0011\nDS0012\nDS0013\nDS0014\nDS0015\nDS0016\nDS0017\nDS0101\nDS0102\nDS0103\nDS0104\nDS0105\nDS0106\nDS0107\nDS0108\nDS0109\nDS0110\nDS0111\nDS0113\nDS0114\nDS0115\nDS0116\nDS0117\nDS0118\nDS0119\nDS0120\nDS0121\nDS0122\nDS0123\nDS0124\nDS0125\nDS0126\nDS0127\nDS0203\nDS0204\nDS0206\nDS0207\nDS0208\nDS0209\nDS0210\nDS0211\nDS0212\nDS0214\nDS0215\nDS0216\nDS0217\nDS0218\nDS0219\nDS0220\nDS0221\nDS0222\nDS0223\nDS0224\nDS0225\nDS0226\nDS0227\nDS0228\nDS0229\nDS0230\nDS0231\nDS0232\nDS0233\nDatasets with consistent number of columns\nDS0001\nDS0112\nDS0201\nDS0202\nDS0205\nDS0213\n\n\n\nname = \"DS0001\"\ni=0\n\nwhile i &lt; len(dataframes[name].columns):\n    if dataframes[name].columns[i] in dataframes2[name].columns:\n        i += 1\n    else:\n        break\n\nif i == len(dataframes[name].columns):\n    print(\"True\")\nelse:\n    print(\"False\")\n\nFalse\n\n\n\ndataframes[name].columns,dataframes2[name].columns\n\n(Index(['SEQN', 'SDDSRVYR', 'RIDSTATR', 'RIDEXMON', 'RIAGENDR', 'RIDAGEYR',\n        'RIDAGEMN', 'RIDAGEEX', 'RIDRETH1', 'DMQMILIT', 'DMDBORN', 'DMDCITZN',\n        'DMDYRSUS', 'DMDEDUC3', 'DMDEDUC2', 'DMDSCHOL', 'DMDMARTL', 'DMDHHSIZ',\n        'DMDFMSIZ', 'INDHHINC', 'INDFMINC', 'INDFMPIR', 'RIDEXPRG', 'DMDHRGND',\n        'DMDHRAGE', 'DMDHRBRN', 'DMDHREDU', 'DMDHRMAR', 'DMDHSEDU', 'SIALANG',\n        'SIAPROXY', 'SIAINTRP', 'FIALANG', 'FIAPROXY', 'FIAINTRP', 'MIALANG',\n        'MIAPROXY', 'MIAINTRP', 'AIALANG', 'WTINT2YR', 'WTMEC2YR', 'SDMVPSU',\n        'SDMVSTRA'],\n       dtype='object'),\n Index(['SEQN', 'SDDSRVYR', 'RIDSTATR', 'RIDEXMON', 'RIAGENDR', 'RIDAGEYR',\n        'RIDAGEMN', 'RIDAGEEX', 'RIDRETH1', 'DMQMILIT', 'DMDBORN2', 'DMDCITZN',\n        'DMDYRSUS', 'DMDEDUC3', 'DMDEDUC2', 'DMDSCHOL', 'DMDMARTL', 'DMDHHSIZ',\n        'DMDFMSIZ', 'INDHHIN2', 'INDFMIN2', 'INDFMPIR', 'RIDEXPRG', 'DMDHRGND',\n        'DMDHRAGE', 'DMDHRBR2', 'DMDHREDU', 'DMDHRMAR', 'DMDHSEDU', 'SIALANG',\n        'SIAPROXY', 'SIAINTRP', 'FIALANG', 'FIAPROXY', 'FIAINTRP', 'MIALANG',\n        'MIAPROXY', 'MIAINTRP', 'AIALANG', 'WTINT2YR', 'WTMEC2YR', 'SDMVPSU',\n        'SDMVSTRA'],\n       dtype='object'))\n\n\nColumns are almost identical aside from DMDBORN and DMDBORN2\nLet’s see how many participants we have from both surveys\n\nICPSR_25504 = pd.read_csv(\"ICPSR_25504\\\\DS0001\\\\25504-0001-Data.tsv\", sep = \"\\t\")\nICPSR_25505 = pd.read_csv(\"ICPSR_25505\\\\DS0001\\\\25505-0001-Data.tsv\", sep = \"\\t\")\nICPSR_25504[\"SEQN\"].apply(lambda x: x in ICPSR_25505[\"SEQN\"].values).sum()\n\n0\n\n\nThere is no way of identifying previous participants in new surverys. There also seems to be little value in analyzing multiple surveys because we can’t create variables that look at year to year changes in the same patient and the data architecture is not consistent across years. The only potential uses of having surveys from different years is to see if a model built on data from one year generalizes to the survey data from other years. In order to do some we must choose variables and datasets that exist in all surveys.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#joining-datasets",
    "href": "Applications/Application1/EDA.html#joining-datasets",
    "title": "6  Data Pre-Processing",
    "section": "6.6 Joining Datasets",
    "text": "6.6 Joining Datasets\n\nimport pandas as pd\nDS0132= pd.read_csv(\"ICPSR_25504\\\\DS0132\\\\25504-0132-Data.tsv\", sep = \"\\t\")\nDS0123 = pd.read_csv(\"ICPSR_25504\\\\DS0123\\\\25504-0123-Data.tsv\", sep = \"\\t\")\njoined = DS0132.set_index(\"SEQN\").join(DS0123.set_index(\"SEQN\"), lsuffix=\"_DS0132\", rsuffix=\"_DS0123\")\n\n\njoined\n\n\n\n\n\n\n\n\nWTSAF2YR_DS0132\nLBXTR\nLBDTRSI\nLBDLDL\nLBDLDLSI\nLBXAPB\nLBDAPBSI\nSDDSRVYR_DS0132\nRIDSTATR_DS0132\nRIDEXMON_DS0132\n...\nFIAPROXY_DS0123\nFIAINTRP_DS0123\nMIALANG_DS0123\nMIAPROXY_DS0123\nMIAINTRP_DS0123\nAIALANG_DS0123\nWTINT2YR_DS0123\nWTMEC2YR_DS0123\nSDMVPSU_DS0123\nSDMVSTRA_DS0123\n\n\nSEQN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n31130\n0.000\n\n\n\n\n\n\n4\n2\n2\n...\n2\n2\n\n\n\n\n29960.839509\n34030.994786\n2\n46\n\n\n31131\n67556.810\n86\n.971\n49\n1.267\n50\n.5\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n26457.708180\n26770.584605\n1\n48\n\n\n31132\n80193.962\n65\n.734\n75\n1.94\n75\n.75\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n32961.509920\n35315.538900\n2\n52\n\n\n31133\n15668.017\n61\n.689\n81\n2.095\n75\n.75\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n5635.221296\n5920.617679\n1\n51\n\n\n31134\n93399.539\n195\n2.202\n98\n2.534\n111\n1.11\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n43718.506372\n44231.167252\n2\n48\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41465\n21859.428\n81\n.914\n96\n2.483\n84\n.84\n4\n2\n1\n...\n2\n2\n1\n2\n2\n1\n6659.491266\n6975.174925\n1\n49\n\n\n41467\n8000.729\n69\n.779\n80\n2.069\n75\n.75\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n3623.884248\n3780.835721\n2\n56\n\n\n41471\n16274.316\n89\n1.005\n72\n1.862\n60\n.6\n4\n2\n1\n...\n2\n2\n1\n2\n2\n1\n6283.305550\n6602.173657\n1\n52\n\n\n41472\n175395.280\n120\n1.355\n86\n2.224\n91\n.91\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n67347.579152\n69747.141506\n1\n48\n\n\n41474\n17122.983\n88\n.994\n35\n.905\n37\n.37\n4\n2\n2\n...\n2\n2\n1\n2\n2\n1\n6174.551667\n6487.262494\n1\n47\n\n\n\n\n3352 rows × 98 columns\n\n\n\nLet’s see if the columns that occur in both dataframes are redundant\n\n(joined[\"AIALANG_DS0123\"] == joined[\"AIALANG_DS0132\"]).mean()\n# AIALANG is definitely redundant\n\n1.0\n\n\n\nDS1 = \"DS0123\"\nDS2 = \"DS0132\"\n\nn = 0\ntotal = 0\nfor column in joined.columns:\n\n    if \"_\" in column:\n        total +=1\n        original = column.split(\"_\")[0]\n        n1 = original + \"_\" + DS1\n        n2 = original + \"_\" + DS2\n\n        if (joined[n1]==joined[n2]).mean() == 1:\n            n += 1\n            joined.drop(n2, axis = 1)\n\nprint(str(n/total) + \" of the columns that occur in both datasets are redundant\")\n        \n\n1.0 of the columns that occur in both datasets are redundant\n\n\n\nimport os\nimport pandas as pd\n\ndata_master = \"ICPSR_25504\\\\\"\ni = 0\njoined_master = DS0123.set_index(\"SEQN\")\n\nfor root, dirs, files in os.walk(data_master):\n    for name in files:\n        if name.endswith(\".tsv\"):\n            file = os.path.join(root, name)\n            df = pd.read_csv(file, sep = \"\\t\")\n            if \"SEQN\" not in df.columns:\n                continue\n            name = root[-6:]\n        \n            new_columns = df.columns.difference(joined_master.columns)\n            # try:\n            joined_master = joined_master.join(df.loc[:,new_columns].set_index(\"SEQN\"),how=\"left\",lsuffix=\"_master\", rsuffix=\"_\"+name)\n            # except:\n            #     print(name)\n            # n = 0\n            # total = 0\n            # for column in joined_master.columns:\n            #     if \"_\" in column:\n            #         print(column, name)\n            #         total +=1\n            #         original = column.split(\"_\")[0]\n            #         n1 = original + \"_master\"\n            #         n2 = original + \"_\" + name\n\n            #         if n2 not in joined_master.columns:\n            #             continue\n\n            #         if (joined_master[n1]==joined_master[n2]).mean() == 1:\n            #             n += 1\n            #             joined_master.drop(n2, axis = 1)\n            #             joined_master.rename({n1:original})        \n            # if n/total != 1:\n            #     print(name + \" - not redundant\")\n            \n\nMemoryError: Unable to allocate 1.30 GiB for an array with shape (222, 786976) and data type float64\n\n\n\njoined_master\n\n\n\n\n\n\n\n\nWTSAF2YR\nLBXGLU\nLBDGLUSI\nLBXIN\nLBDINSI\nPHAFSTHR\nPHAFSTMN\nSDDSRVYR\nRIDSTATR\nRIDEXMON\n...\nDXXNKBMD\nDXXOFA\nDXXOFBMC\nDXXOFBMD\nDXXTRA\nDXXTRBMC\nDXXTRBMD\nDXXWDA\nDXXWDBMC\nDXXWDBMD\n\n\nSEQN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n31130\n0.000\n\n\n\n\n10\n3\n4\n2\n2\n...\n\n\n\n\n\n\n\n\n\n\n\n\n31131\n67556.810\n90\n4.996\n10.03\n60.18\n14\n9\n4\n2\n2\n...\n\n\n\n\n\n\n\n\n\n\n\n\n31131\n67556.810\n90\n4.996\n10.03\n60.18\n14\n9\n4\n2\n2\n...\n\n\n\n\n\n\n\n\n\n\n\n\n31131\n67556.810\n90\n4.996\n10.03\n60.18\n14\n9\n4\n2\n2\n...\n\n\n\n\n\n\n\n\n\n\n\n\n31131\n67556.810\n90\n4.996\n10.03\n60.18\n14\n9\n4\n2\n2\n...\n\n\n\n\n\n\n\n\n\n\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41474\n17122.983\n93\n5.162\n20.12\n120.72\n9\n39\n4\n2\n2\n...\n.825\n38.34\n30.73\n.802\n12.1\n6.63\n.548\n1.3\n.96\n.736\n\n\n41474\n17122.983\n93\n5.162\n20.12\n120.72\n9\n39\n4\n2\n2\n...\n.825\n38.34\n30.73\n.802\n12.1\n6.63\n.548\n1.3\n.96\n.736\n\n\n41474\n17122.983\n93\n5.162\n20.12\n120.72\n9\n39\n4\n2\n2\n...\n.825\n38.34\n30.73\n.802\n12.1\n6.63\n.548\n1.3\n.96\n.736\n\n\n41474\n17122.983\n93\n5.162\n20.12\n120.72\n9\n39\n4\n2\n2\n...\n.825\n38.34\n30.73\n.802\n12.1\n6.63\n.548\n1.3\n.96\n.736\n\n\n41474\n17122.983\n93\n5.162\n20.12\n120.72\n9\n39\n4\n2\n2\n...\n.825\n38.34\n30.73\n.802\n12.1\n6.63\n.548\n1.3\n.96\n.736\n\n\n\n\n786976 rows × 742 columns\n\n\n\n\n# The data is far too large to combine into one so I will choose critical datasets to focus the analysis on",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#ds209---depression-screener",
    "href": "Applications/Application1/EDA.html#ds209---depression-screener",
    "title": "6  Data Pre-Processing",
    "section": "6.7 DS209 - Depression Screener",
    "text": "6.7 DS209 - Depression Screener\nThis data set is a Questionnaire with 10 questions about depressive symptoms (Eg. Thought you would be better off dead.) The response are categorical from 0-3 with 0 being non-depressive behaviour (Not at all) and 4 being depressive behaviour (Nearly every day).\nWe will create a target variable that aggregate the results of these questions, taking the mean response of the questions which scores a participants depressive state. 0 meaning low, 4 high\nThe respondant can answer a question as “Don’t know” and will be assigned the number 9 which can throw off our scores. I will remove these values from the average calculations. So for example, if a respondant answers “Don’t know” for 2 questions, their score will be the average of their repsonses to the other 8 questions.\n\nDS209= pd.read_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"\\\\Data\\\\NHANES\\\\ICPSR_25504\\DS0209\\\\25504-0209-Data.tsv\", sep = \"\\t\")\n\n\nDS209.dtypes\n\nSEQN          int64\nDPQ010       object\nDPQ020       object\nDPQ030       object\nDPQ040       object\nDPQ050       object\nDPQ060       object\nDPQ070       object\nDPQ080       object\nDPQ090       object\nDPQ100       object\nSDDSRVYR      int64\nRIDSTATR      int64\nRIDEXMON      int64\nRIAGENDR      int64\nRIDAGEYR      int64\nRIDAGEMN     object\nRIDAGEEX     object\nRIDRETH1      int64\nDMQMILIT      int64\nDMDBORN       int64\nDMDCITZN      int64\nDMDYRSUS     object\nDMDEDUC3     object\nDMDEDUC2     object\nDMDSCHOL     object\nDMDMARTL      int64\nDMDHHSIZ      int64\nDMDFMSIZ      int64\nINDHHINC     object\nINDFMINC     object\nINDFMPIR     object\nRIDEXPRG     object\nDMDHRGND      int64\nDMDHRAGE      int64\nDMDHRBRN     object\nDMDHREDU     object\nDMDHRMAR     object\nDMDHSEDU     object\nSIALANG      object\nSIAPROXY     object\nSIAINTRP     object\nFIALANG      object\nFIAPROXY     object\nFIAINTRP     object\nMIALANG      object\nMIAPROXY     object\nMIAINTRP     object\nAIALANG      object\nWTINT2YR    float64\nWTMEC2YR    float64\nSDMVPSU       int64\nSDMVSTRA      int64\ndtype: object\n\n\nWe need to turn some columns into integer\n\nimport numpy as np\nDS209 = DS209.replace(\" \", np.NaN )\n\n\nDS209[DS209.columns[1:11]]\nconvert_dict = {}\nfor column in DS209.columns[1:11]:\n    DS209[column] = pd.to_numeric(DS209[column], errors='coerce')\nprint(DS209.dtypes)\n\nSEQN          int64\nDPQ010      float64\nDPQ020      float64\nDPQ030      float64\nDPQ040      float64\nDPQ050      float64\nDPQ060      float64\nDPQ070      float64\nDPQ080      float64\nDPQ090      float64\nDPQ100      float64\nSDDSRVYR      int64\nRIDSTATR      int64\nRIDEXMON      int64\nRIAGENDR      int64\nRIDAGEYR      int64\nRIDAGEMN     object\nRIDAGEEX     object\nRIDRETH1      int64\nDMQMILIT      int64\nDMDBORN       int64\nDMDCITZN      int64\nDMDYRSUS     object\nDMDEDUC3     object\nDMDEDUC2     object\nDMDSCHOL     object\nDMDMARTL      int64\nDMDHHSIZ      int64\nDMDFMSIZ      int64\nINDHHINC     object\nINDFMINC     object\nINDFMPIR     object\nRIDEXPRG     object\nDMDHRGND      int64\nDMDHRAGE      int64\nDMDHRBRN     object\nDMDHREDU     object\nDMDHRMAR     object\nDMDHSEDU     object\nSIALANG      object\nSIAPROXY     object\nSIAINTRP     object\nFIALANG      object\nFIAPROXY     object\nFIAINTRP     object\nMIALANG      object\nMIAPROXY     object\nMIAINTRP     object\nAIALANG      object\nWTINT2YR    float64\nWTMEC2YR    float64\nSDMVPSU       int64\nSDMVSTRA      int64\ndtype: object\n\n\n\n(DS209.iloc[:, 1:11].isna().mean(axis = 1) == 1).sum()\n\n498\n\n\n498 participants did not respond at all. Let’s remove these from the dataset\n\nclean = DS209[DS209.iloc[:, 1:11].isnull().mean(axis = 1) != 1]\n\n\ndepression_score = clean.iloc[:, 1:11].mean(axis = 1)\n\n\nfinal = pd.concat([clean, depression_score], axis = 1).rename(columns = {0:\"DepressionScore\"}).set_index(\"SEQN\")",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/EDA.html#distribution-of-target-variable",
    "href": "Applications/Application1/EDA.html#distribution-of-target-variable",
    "title": "6  Data Pre-Processing",
    "section": "6.8 Distribution of target variable",
    "text": "6.8 Distribution of target variable\n\nfinal[\"DepressionScore\"].describe()\n\ncount    4836.000000\nmean        0.306493\nstd         0.437908\nmin         0.000000\n25%         0.000000\n50%         0.100000\n75%         0.400000\nmax         4.900000\nName: DepressionScore, dtype: float64\n\n\n\nimport matplotlib.pyplot as plt\nfrom math import log\n\nplt.hist(final[\"DepressionScore\"], bins = 15)\nplt.title(\"Distribution of Depression Scores\")\nplt.xlabel(\"Depression Score\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfinal.to_csv(os.path.dirname(os.path.dirname(os.getcwd()))+\"/Data/NHANES/depression_table.csv\")",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Pre-Processing</span>"
    ]
  },
  {
    "objectID": "Applications/Application1/Elementary_Modeling.html",
    "href": "Applications/Application1/Elementary_Modeling.html",
    "title": "7  Blood lead and Blood Cadmium",
    "section": "",
    "text": "7.1 Model Fitting\nfrom sklearn.model_selection import train_test_split\n\ntrainX, testX, trainy, testy = train_test_split(master.dropna().drop(\"DepressionScore\", axis = 1), master.dropna()[\"DepressionScore\"], test_size=0.2)\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression().fit(trainX, trainy)\npreds_lm = lm.predict(testX)\nlm.score(trainX, trainy)\n\n0.05273780957104557\nR^2 is 0.05 - linear model preforms very poorly. Either the linear model poorly measures a non-linear relationship or the predictors contain very little information about the mental health of our participants. Let’s try a tree-based method:\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(max_depth = 2)\nrf.fit(trainX, trainy)\npreds_rf = rf.predict(testX)\nrf.score(trainX, trainy)\n\n0.060034486182498625\nR^2 is 0.06 - our coefficient of determination only improves slightly with a non-linear model. This would lead me to conclude that our predictors contain no information about the mental health of our participants.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Fitting</span>"
    ]
  },
  {
    "objectID": "Applications/Application2/boosting.html",
    "href": "Applications/Application2/boosting.html",
    "title": "8  Boosting on CGC Data",
    "section": "",
    "text": "8.1 Pre-processing\nfrom sklearn.model_selection import train_test_split\n# Drop unknown vital status\ndf2 = df[df[\"vital_status\"] != \"Unknown\"]\nX = df.drop(columns = [\"disease_type\", \"vital_status\"])\nbinary_y = df[\"vital_status\"].apply(lambda x: 1 if x == \"Alive\" else 0)\nmulticlass_y = df[\"disease_type\"]\nX_train, X_val, binary_y_train, binary_val = train_test_split(X, binary_y, train_size=0.8, random_state=4)\nX_train2, X_test, binary_y_train2, binary_y_test = train_test_split(X_train, binary_y_train, train_size=0.7, random_state=4)\n#X_train, X_test, multiclass_y_train, multiclass_y_test = train_test_split(X, multiclass_y, train_size=0.7, random_state=4)",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosting on CGC Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application2/boosting.html#binary-ada-boost",
    "href": "Applications/Application2/boosting.html#binary-ada-boost",
    "title": "8  Boosting on CGC Data",
    "section": "8.2 Binary ADA Boost",
    "text": "8.2 Binary ADA Boost\nHere we will be using miRNA counts as predictors and vital status (aka whether the patient has survived the cancer (1) or not (0)) as the target.\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nbinary_classifier = AdaBoostClassifier()\nbinary_classifier.fit(X_train2, binary_y_train2)\n\nAdaBoostClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifierAdaBoostClassifier()\n\n\n\n(binary_classifier.decision_function(X_test) &gt;= 0)[:5]\n\narray([False, False,  True,  True,  True])\n\n\n\nbinary_classifier.predict(X_test)[:5]\n\narray([0, 0, 1, 1, 1], dtype=int64)\n\n\nThe decision function essentially gives us the weighted average of the weak classifiers whereas predict returns the sign of the decision function, which in the end is the class predicton.\n\ndef cm_mapper(actual, predictions):\n    import numpy as np\n    mapping = []\n    for i in range(len(predictions)):\n\n        if predictions[i]:\n            if actual[i]:\n                mapping.append(\"TP\")\n            else:\n                mapping.append('FP')\n        else:\n            if actual[i]:\n                mapping.append('FN')\n            else:\n                mapping.append('TN')\n\n    return np.array(mapping)\n\n\nimport pandas as pd\nweights = binary_classifier.decision_function(X_test)\npredictions1 = binary_classifier.predict(X_test)\ncm = cm_mapper(predictions1, binary_y_test)\n\nresults = pd.DataFrame({'Actual':binary_y_test, 'Predicted':predictions1, 'Weight':weights, \"Outcome\":cm})\n\n\nimport matplotlib.pyplot as plt\ngroups = results.groupby('Outcome')\nfor name, group in groups:\n    plt.hist(abs(group.Weight), density=True)\n    plt.title(\"Distribution of Decision Function Values for \" + name)\n    plt.xlabel(\"Decision Function Magnitude\")\n    plt.ylabel(\"Density\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf the magnitude of the decision function is high then you would expect that the model is very sure about the decision it is making. I would assume that these values would surely be true positives and true negatives. Looking at the graphs, it turns out that this assumption is not necessarily true\n\nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, threshold = roc_curve(binary_y_test, predictions1)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix, cohen_kappa_score\n\nprint(\"Confusion Matrix:\\n\", confusion_matrix(binary_y_test, predictions1)) \nprint(\"Accuracy = \", binary_classifier.score(X_test, binary_y_test))\nprint(\"Kappa = \", cohen_kappa_score(binary_y_test, predictions1))\n\nConfusion Matrix:\n [[34 27]\n [32 75]]\nAccuracy =  0.6488095238095238\nKappa =  0.25383920505871727\n\n\nOur confusion matrix shows a poor accuracy but our kappa value is not awful.\nThe first attempt at boosting has not resulted in a very successful model.\n\nHyperparameter Tuning\n\nfrom sklearn.model_selection import cross_val_score\n\n\n# Number of estimators\n\nestimators = [10,50,100,150,200]\nscore = []\n\nfor n in estimators:\n\n    classifier = AdaBoostClassifier(n_estimators= n)\n    score.append(cross_val_score(classifier, X_train2, binary_y_train2).mean())\n    \n\n\nplt.plot(estimators, score)\nplt.title(\"Relationship Between Accuracy and Number Estimators\")\nplt.ylabel(\"Cross Validation Accuracy\")\nplt.xlabel(\"Number of Estimators in Boosting Algorithm\")\nplt.show()\n\n\n\n\n\n\n\n\nWe increase our accuracy by roughly 0.01% per estimator which means (assuming linear effects) we would have to add 1000 estimators to get our accuracy from 60% to 70%.\n\n# Learning rate\n\nrates = [0.1,0.25,0.5,0.75,1]\nscore = []\n\nfor r in rates:\n\n    classifier = AdaBoostClassifier(learning_rate=r)\n    score.append(cross_val_score(classifier, X_train2, binary_y_train2).mean())\n\nplt.plot(rates, score)\nplt.title(\"Relationship Between Accuracy and Learning Rate\")\nplt.ylabel(\"Cross Validation Accuracy\")\nplt.xlabel(\"Learning Rate of Boosting Algorithm\")\nplt.show()\n\n\n\n\n\n\n\n\nLowering learning rates give us a boost in accuracy. For our final model we will use a learning rate of 0.1\n\ndef plot_roc(real, predicted, lab):\n    from sklearn.metrics import roc_curve, auc\n    fpr, tpr, threshold2 = roc_curve(real, predicted)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label = lab + ' AUC = %0.2f' % roc_auc)\n    \n\n\n# SAMME vs SAMME.N\n\nbinary_classifier2 = AdaBoostClassifier(algorithm=\"SAMME\")\nbinary_classifier2.fit(X_train2, binary_y_train2)\n\npredictions2 = binary_classifier2.predict(X_test)\n\nplot_roc(binary_y_test, predictions1, \"SAMME.R\")\nplot_roc(binary_y_test, predictions2, \"SAMME\")\n\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(\"SAMME Results\")\n\nprint(\"Confusion Matrix:\\n\", confusion_matrix(binary_y_test, predictions2)) \nprint(\"Accuracy = \", binary_classifier2.score(X_test, binary_y_test))\nprint(\"Kappa = \", cohen_kappa_score(binary_y_test, predictions2))\n\nSAMME Results\nConfusion Matrix:\n [[27 34]\n [32 75]]\nAccuracy =  0.6071428571428571\nKappa =  0.14457645425088717\n\n\nResults are similar. SAMME.R outperforms slightly\n\n# Different classifier \nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.linear_model import RidgeClassifier, LogisticRegression\n\nRidge_booster = AdaBoostClassifier(estimator=RidgeClassifier(),algorithm=\"SAMME\")\nRidge_booster.fit(X_train2, binary_y_train2)\npredictions3 = Ridge_booster.predict(X_test)\n\nLogistic_booster = AdaBoostClassifier(estimator=LogisticRegression(max_iter=100))\nLogistic_booster.fit(X_train2, binary_y_train2)\npredictions4 = Logistic_booster.predict(X_test)\n\nplot_roc(binary_y_test, predictions1, \"Decision Tree\")\nplot_roc(binary_y_test, predictions3, \"Ridge\")\nplot_roc(binary_y_test, predictions4, \"Logistic\")\n\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n\n\n\n\n\n\n\nThe decision tree outperforms the linear classifiers. Let’s see if adding more depth to the decisions trees improves our accuracy (at the loss of interpretibility)\n\nfrom sklearn.tree import DecisionTreeClassifier\nbinary_classifier3 = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2))\nbinary_classifier3.fit(X_train2, binary_y_train2)\n\npredictions5 = binary_classifier3.predict(X_test)\n\nplot_roc(binary_y_test, predictions1, \"Stump\")\nplot_roc(binary_y_test, predictions5, \"Depth 2 Tree\")\n\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n\n\n\n\n\n\n\nAdding depth to the decision tree classifier does not improve the prediction accuracy. After all the hyperparameter tuning, our original model remains strong aside from the an reduction in the learning rate.\n\nfinal_model = AdaBoostClassifier(learning_rate=0.1)\nfinal_model.fit(X_train, binary_y_train)\n\nfinal_predictions = final_model.predict(X_val)\nfrom sklearn.metrics import confusion_matrix, cohen_kappa_score\n\nprint(\"Final Model Results\")\nprint(\"Confusion Matrix:\\n\", confusion_matrix(binary_val, final_predictions)) \nprint(\"Accuracy = \", binary_classifier.score(X_val, binary_val))\nprint(\"Kappa = \", cohen_kappa_score(binary_val, final_predictions))\n\nFinal Model Results\nConfusion Matrix:\n [[ 7 47]\n [ 9 77]]\nAccuracy =  0.6\nKappa =  0.0287413280475719\n\n\n\n# Visualization taken from sklearn\n# https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py\n\n\ntwoclass_output = final_model.decision_function(X)\nplot_range = (twoclass_output.min(), twoclass_output.max())\nplt.subplot(122)\nclass_names = [\"Dead\", \"Alive\"]\nplot_colors = \"br\"\nfor i, n, c in zip(range(2), class_names, plot_colors):\n    plt.hist(\n        twoclass_output[binary_y == i],\n        bins=10,\n        range=plot_range,\n        facecolor=c,\n        label=n,\n        alpha=0.5,\n        edgecolor=\"k\",\n    )\nx1, x2, y1, y2 = plt.axis()\nplt.axis((x1, x2, y1, y2 * 1.2))\nplt.legend(loc=\"upper right\")\nplt.ylabel(\"Samples\")\nplt.xlabel(\"Score\")\nplt.title(\"Decision Scores\")\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.35)\nplt.show()",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosting on CGC Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application2/boosting.html#variable-importance",
    "href": "Applications/Application2/boosting.html#variable-importance",
    "title": "8  Boosting on CGC Data",
    "section": "8.3 Variable Importance",
    "text": "8.3 Variable Importance\n\ntop10_important = pd.DataFrame({\"miRNA_id\":final_model.feature_names_in_, \"Importance\":final_model.feature_importances_}).sort_values(\"Importance\", ascending=False).head(10)\nplt.barh(top10_important[\"miRNA_id\"], top10_important[\"Importance\"])\nplt.title(\"Top 10 miRNA by importance\")\nplt.xlabel(\"miRNA ID\")\nplt.xlabel(\"Feature Importance\")\nplt.show()",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosting on CGC Data</span>"
    ]
  },
  {
    "objectID": "Applications/Application2/zero_inflated_model.html",
    "href": "Applications/Application2/zero_inflated_model.html",
    "title": "9  NHANES Extension to Zero Inflated Models",
    "section": "",
    "text": "9.1 Previous Models\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n:::\n::: {.cell-output .cell-output-stdout}\n::: :::",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>NHANES Extension to Zero Inflated Models</span>"
    ]
  },
  {
    "objectID": "Applications/Application2/zero_inflated_model.html#previous-models",
    "href": "Applications/Application2/zero_inflated_model.html#previous-models",
    "title": "9  NHANES Extension to Zero Inflated Models",
    "section": "",
    "text": "# Fit lm and rf\nmodel &lt;- lm(DepressionScore ~ ., data = train2)\nsummary(model)\n\n\nCall:\nlm(formula = DepressionScore ~ ., data = train2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6836 -0.2523 -0.1369  0.1111  3.6225 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.1515982  0.8326350   0.182   0.8555    \nLBXBCD      37.8102911 25.3862173   1.489   0.1365    \nLBDBCDSI    -4.2442331  2.8536336  -1.487   0.1371    \nLBXBPB       0.0008153  0.0054061   0.151   0.8801    \nLBXWBCSI    -0.0440171  0.0831151  -0.530   0.5964    \nLBDLYMNO     0.0576497  0.0837127   0.689   0.4911    \nLBDMONO      0.0212968  0.0943325   0.226   0.8214    \nLBDNENO      0.0671081  0.0830492   0.808   0.4191    \nLBDEONO      0.0231188  0.0971888   0.238   0.8120    \nLBDBANO     -0.1554687  0.1608713  -0.966   0.3339    \nLBXRBCSI    -0.1380744  0.1664479  -0.830   0.4069    \nLBXHGB       0.0341865  0.0551472   0.620   0.5354    \nLBXMCHSI    -0.0133475  0.0263445  -0.507   0.6124    \nLBXRDW       0.0344879  0.0086260   3.998 6.59e-05 ***\nLBXPLTSI     0.0003369  0.0001462   2.305   0.0213 *  \nLBXMPSI      0.0074942  0.0115524   0.649   0.5166    \nLBXCRP       0.0100179  0.0103447   0.968   0.3329    \nLBXVID      -0.0008734  0.0010405  -0.839   0.4014    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4164 on 2231 degrees of freedom\nMultiple R-squared:  0.04218,  Adjusted R-squared:  0.03488 \nF-statistic:  5.78 on 17 and 2231 DF,  p-value: 3.986e-13\n\nrf &lt;- randomForest(DepressionScore~ ., data = train2)\nprint(rf)\n\n\nCall:\n randomForest(formula = DepressionScore ~ ., data = train2) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 5\n\n          Mean of squared residuals: 0.1827745\n                    % Var explained: -1.78",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>NHANES Extension to Zero Inflated Models</span>"
    ]
  },
  {
    "objectID": "Applications/Application2/zero_inflated_model.html#multicollinearity",
    "href": "Applications/Application2/zero_inflated_model.html#multicollinearity",
    "title": "9  NHANES Extension to Zero Inflated Models",
    "section": "9.2 Multicollinearity",
    "text": "9.2 Multicollinearity\nThere seem to be problems with multicollinearity - lets check vifs.\n::: {.cell}\nlibrary(car)\nvif(model)\n::: {.cell-output .cell-output-stdout}\n      LBXBCD     LBDBCDSI       LBXBPB     LBXWBCSI     LBDLYMNO      LBDMONO \n3.357160e+06 3.357132e+06 1.122255e+00 4.762114e+02 7.701938e+01 4.736069e+00 \n     LBDNENO      LBDEONO      LBDBANO     LBXRBCSI       LBXHGB     LBXMCHSI \n3.212382e+02 3.351542e+00 1.228121e+00 1.017545e+02 1.021855e+02 4.686998e+01 \n      LBXRDW     LBXPLTSI      LBXMPSI       LBXCRP       LBXVID \n1.454438e+00 1.447789e+00 1.295587e+00 1.156630e+00 1.100999e+00 \n:::\n# LBXBCD very high\n\nmodel &lt;- lm(DepressionScore ~ . -LBXBCD , data = train2) \nvif(model)\n::: {.cell-output .cell-output-stdout}\n  LBDBCDSI     LBXBPB   LBXWBCSI   LBDLYMNO    LBDMONO    LBDNENO    LBDEONO \n  1.117626   1.121438 475.625335  76.916818   4.734852 320.806221   3.348682 \n   LBDBANO   LBXRBCSI     LBXHGB   LBXMCHSI     LBXRDW   LBXPLTSI    LBXMPSI \n  1.228088 101.727731 102.149769  46.851618   1.451933   1.446682   1.295297 \n    LBXCRP     LBXVID \n  1.156594   1.100722 \n:::\n# LBXWBCSI very high\nmodel &lt;- lm(DepressionScore ~ . -LBXWBCSI -LBXBCD , data = train2) \nvif(model)\n::: {.cell-output .cell-output-stdout}\n  LBDBCDSI     LBXBPB   LBDLYMNO    LBDMONO    LBDNENO    LBDEONO    LBDBANO \n  1.117390   1.121006   1.159928   1.290370   1.288243   1.081915   1.095463 \n  LBXRBCSI     LBXHGB   LBXMCHSI     LBXRDW   LBXPLTSI    LBXMPSI     LBXCRP \n101.614682 102.046096  46.813900   1.451836   1.443688   1.293852   1.155949 \n    LBXVID \n  1.100529 \n:::\n# LBXRBCSI still very high\nmodel &lt;- lm(DepressionScore ~ . - LBXRBCSI -LBXWBCSI -LBXBCD , data = train2) \nvif(model)\n::: {.cell-output .cell-output-stdout}\nLBDBCDSI   LBXBPB LBDLYMNO  LBDMONO  LBDNENO  LBDEONO  LBDBANO   LBXHGB \n1.116640 1.120960 1.159879 1.289519 1.287440 1.080900 1.094918 1.324159 \nLBXMCHSI   LBXRDW LBXPLTSI  LBXMPSI   LBXCRP   LBXVID \n1.396214 1.448955 1.440443 1.293106 1.155940 1.100420 \n:::\n# Now VIFs are pretty good\nrf &lt;- randomForest(DepressionScore~ . - LBXRBCSI -LBXWBCSI -LBXBCD, data = train2)\nprint(rf)\n::: {.cell-output .cell-output-stdout}\n\nCall:\n randomForest(formula = DepressionScore ~ . - LBXRBCSI - LBXWBCSI -      LBXBCD, data = train2) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 0.1820826\n                    % Var explained: -1.39\n::: :::",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>NHANES Extension to Zero Inflated Models</span>"
    ]
  },
  {
    "objectID": "Applications/Application2/zero_inflated_model.html#zero-inflated-model",
    "href": "Applications/Application2/zero_inflated_model.html#zero-inflated-model",
    "title": "9  NHANES Extension to Zero Inflated Models",
    "section": "9.3 Zero Inflated Model",
    "text": "9.3 Zero Inflated Model\n::: {.cell}\n# Turn depression score into even counts\ntrain$DepressionScore&lt;-floor(train$DepressionScore)\ntrain2$DepressionScore&lt;-floor(train2$DepressionScore)\ntest$DepressionScore&lt;-floor(test$DepressionScore)\nval$DepressionScore&lt;-floor(val$DepressionScore)\nmaster2$DepressionScore&lt;-floor(master2$DepressionScore)\n\nmaster2  |&gt; filter(DepressionScore &gt; 1)  |&gt;  ggplot(aes(DepressionScore)) + geom_density() + ggtitle(\"Distribution of Non-Zero Values\")\n::: {.cell-output-display}  ::: :::\n::: {.cell}\nlibrary(pscl)\nzim &lt;- zeroinfl(DepressionScore ~ . - LBXRBCSI -LBXWBCSI -LBXBCD | . - LBXRBCSI -LBXWBCSI -LBXBCD, data = train2, dist=\"poisson\")\nsummary(zim)\n::: {.cell-output .cell-output-stdout}\n\nCall:\nzeroinfl(formula = DepressionScore ~ . - LBXRBCSI - LBXWBCSI - LBXBCD | \n    . - LBXRBCSI - LBXWBCSI - LBXBCD, data = train2, dist = \"poisson\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.8010 -0.3227 -0.2527 -0.1827 10.0520 \n\nCount model coefficients (poisson with log link):\n              Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -3.8569902  2.5065300  -1.539  0.12386   \nLBDBCDSI    -0.0028849  0.0183216  -0.157  0.87488   \nLBXBPB      -0.0227193  0.0518124  -0.438  0.66103   \nLBDLYMNO     0.0709681  0.0795544   0.892  0.37236   \nLBDMONO     -0.8205831  0.6240312  -1.315  0.18852   \nLBDNENO      0.1556227  0.0542358   2.869  0.00411 **\nLBDEONO      1.1530529  0.5438638   2.120  0.03400 * \nLBDBANO     -2.0907086  1.9292933  -1.084  0.27851   \nLBXHGB      -0.0499782  0.0787098  -0.635  0.52545   \nLBXMCHSI     0.0469841  0.0501544   0.937  0.34887   \nLBXRDW       0.0645939  0.0600753   1.075  0.28228   \nLBXPLTSI     0.0002716  0.0016448   0.165  0.86885   \nLBXMPSI      0.0111427  0.1444323   0.077  0.93851   \nLBXCRP       0.0769067  0.0906671   0.848  0.39631   \nLBXVID      -0.0125921  0.0121630  -1.035  0.30054   \n\nZero-inflation model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  3.472576   9.744862   0.356   0.7216  \nLBDBCDSI    -0.490601   0.302405  -1.622   0.1047  \nLBXBPB      -0.416726   0.228825  -1.821   0.0686 .\nLBDLYMNO    -0.379816   0.320773  -1.184   0.2364  \nLBDMONO     -1.559080   2.129434  -0.732   0.4641  \nLBDNENO      0.110673   0.169216   0.654   0.5131  \nLBDEONO      3.420176   2.150366   1.591   0.1117  \nLBDBANO     -0.906057   6.050265  -0.150   0.8810  \nLBXHGB       0.082175   0.202847   0.405   0.6854  \nLBXMCHSI     0.060435   0.146320   0.413   0.6796  \nLBXRDW      -0.170071   0.322236  -0.528   0.5977  \nLBXPLTSI    -0.006914   0.007696  -0.898   0.3690  \nLBXMPSI      0.068362   0.361686   0.189   0.8501  \nLBXCRP       0.333283   0.309059   1.078   0.2809  \nLBXVID      -0.022947   0.037302  -0.615   0.5384  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 101 \nLog-likelihood: -662.9 on 30 Df\n::: :::\n::: {.cell}\n# Evaluate models\nrf_preds &lt;- predict(rf, test)\nlm_preds &lt;- predict(model, test)\nzim_preds &lt;- predict(zim, test)\n\nlm_mse&lt;-sum((lm_preds - test$DepressionScore)^2)/length(lm_preds)\nrf_mse&lt;-sum((rf_preds - test$DepressionScore)^2)/length(rf_preds)\nzim_mse&lt;-sum((zim_preds - test$DepressionScore)^2)/length(zim_preds)\n:::\n\nprint(\"Linear Model MSE: \")\n\n[1] \"Linear Model MSE: \"\n\nprint(lm_mse)\n\n[1] 0.1551185\n\nprint(\"Random Forest MSE: \")\n\n[1] \"Random Forest MSE: \"\n\nprint(rf_mse)\n\n[1] 0.1711746\n\nprint(\"Zero Inflation Model MSE: \") \n\n[1] \"Zero Inflation Model MSE: \"\n\nprint(zim_mse)\n\n[1] 0.1202842\n\n\nLinear model still has a better MSE than the zero-inflation model. A key observation that I made when fitting the model is that the zero-inflation model assumes the non-zero component model is poisson. This would mean that the target variable is a count among other things such as equal mean and variance. This is something that can be explored further in the next application.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>NHANES Extension to Zero Inflated Models</span>"
    ]
  }
]